# Introduction
Evaluation represents a comprehensive understanding to related characteristics of the producer.

One key advantage of human product is the reputation to be observed.

Reputation shows two sides: looking backwards and predicting future.

People have recognized that we have different opinions towards product that are made by human and made by algorithm or AI.

There is different pattern when it comes to follow the advice given by human and AI.

The dichotomy of convention judgement about human product or AI product seems to be reasonable.

However, we do see that AI therapist is getting popular and it was underestimated when the Generative AI first came up.

If we follow the logic of ‘emotion’ versus ‘rational’ choice different, we should expect them to be wary about the use of AI in giving them advice on their personal relationship and the deep emotions.

The standing ground here is that people judge AI-generated product beyond the simple quality judgement, but foreseeing the consequences it may caused.

People may devalue the product because it is simply ‘too fast’.

Human beings are prone to use effort heuristics, which links effort with quality of the product.

Even if the quality seems the same, people may think about the consequences in the future.

There is a straightforward calling to solve the confusion along with the AI in the work: disclosing the use of AI.

The key audience that I try to focus is the AI transparency literature, which is how and what is the consequence of disclosing the AI use in the work.

In our study, we deliberately choose the structural decision making as our main goal needs AI assist.

While there are more callings about AI product disclaimer, we have little evidence about what will be look like if we are providing a long, structural report.

The report structure is coherent with the McKinsey’s famous SCR method of consulting: Situation, Complication, and Resolution.

[An argument is why I am using the structural report to analyze it.]

Is it commonly used in the business world? Is it fundamental to decision making? ]

Similar to [[@stradiAlgorithmAversionAppreciation2025|(Stradi et al., 2025)]], I include “Human+Machine”, in addition to “Human” and “Machine”.

Disclosing the AI usage may not be favored by producer, because they may lose trust from evaluators. [[@schilkeTransparencyDilemmaHow2025|(Schilke et al., 2025)]]

### Human-AI collaboration is the trend and we should know more about the preference about it
Ethan Mollick's book Co-intelligence points out the “jagged frontier” of AI tools. To complete complicated tasks, it is necessary to have human input in some areas of weakness in AI. 
More and more research discuss about the Human-AI collaboration increases the productivity.
However, our research try to show that the productivity, as somewhat measured objectively, also missed the discussion on how people perceived the AI-assisted products. In lots of industries, 
Asktitas (2025) shows that human-AI collaboration is prevalent in academic writing. 
More research is on the topic. 
Cathy et al. (2025, MS forthcoming) talks about the [[Emails from Andras#2025-08-18 09 13]]
[[@caoManVsMachine2024]] uses AI-human financial analyst and compare the productivity between the two.

### GenAI is causing problem and people are calling for disclosure

[[Anecdote#Deloitte delivers report to government using AI which contained errors ABC NEWS]]
[[Beyond Citation Describing AI Use in Your Work]]



### AI label effects have been tested in...
>It is well documented that new technologies are often met with skepticism. Studies suggest a general sentiment of hesitancy (e.g., von Eschenbach, 2021) and mild to moderate aversion (e.g., Castelo & Ward, 2021; Jussupow et al., 2020) towards AI and computer algorithms more broadly. Also, when told that AI was involved in the creation of communicative content, there was some reporting of preference against or lower evaluation of that content (e.g., Airbnb profile writing; Jakesch et al., 2019; email writing; Liu et al., 2022; generated paintings; Ragot et al., 2020; music creation; Shank et al., 2023; translation of written content; Asscher & Glikson, 2023). Within health contexts especially, some studies show that people tend to prefer human practitioners over AI-based technologies like chatbots when receiving consultation about health conditions (e.g., Miles et al., 2021), citing lack of personalization and incompetence in addressing individual needs as some of the reasons for hesitancy (Longoni et al., 2019).
>For example, Ragot et al. (2020) found that when people perceived AI as the creator of artwork, they evaluated the art more negatively (compared to human-generated artwork) in terms of beauty, novelty, or meaningfulness. Specifically in the context of health messaging, Karinshak et al. (2023) examined how source disclosure impact people’s ratings of health campaigns messages. **(From Lim and Schmälzle, 2025) (Don't plagarize.)**

>In terms of decision-maker preferences between human and AI advisors, some research has found that decision-makers prefer human over AI advice (e.g., [Dietvorst et al., 2015](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref25); [Larkin et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref72)). This is one form of what is often referred to as algorithm aversion, or general negative attitudes and behaviors toward the algorithm ([Logg et al., 2019](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref81); [Lai et al., 2021](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref69)). For example, [Larkin et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref72) found that participants indicated they would prefer to receive recommendations from a human expert versus AI in financial and, even more so, healthcare and contexts. Similarly, [Dietvorst et al. (2015)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref25) found that, across forecasting tasks on student performance and airline performance, decision-makers consistently chose human judgment when choosing between AI forecasts and either their own forecasts or the forecasts of another human participant.
>
>However, other research has found the converse (e.g., [Logg et al., 2019](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref81); [Kennedy et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref62)). For example, [Kennedy et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref62) found that in geopolitical and criminal justice forecasting experiments, decision makers placed a higher weight on AI advice (i.e., forecasting algorithms) relative to several kinds of human advice (i.e., aggregate of expert decision-maker responses; aggregate of non-expert decision-maker responses)–in other words, algorithm appreciation rather than aversion.


### Credibility
Credibility is both a systematic and heuristic evaluation.

Metzger et al. (2010) shows that the credibility is mostly through heuristic/non-deliberate way
>Results also indicate that rather than systematically processing information, participants routinely invoked cognitive heuristics to evaluate the credibility of information and sources online.

### Dilemma of transparency and disclosure
[[@longDisclosureDilemmaHow2025]] found that  message will lose credibility after revealing the AI-label
Our work is different because we only say the AI 'involved'
[[@toffTheyCouldJust2024]]
[[@schilkeTransparencyDilemmaHow2025|Schilke et al. (2025)]]

For these papers, they think about the AI label on message credibility, but they haven't looked into the credibility of the disclosure itself.

One strand of papers are concerned about the AI perception and preference: do people like AI product or human product? Another strand of papers are concerned about the disclosure: do people prefer the product with AI disclosure?

### Contamination effects of AI transparency
For producers, revealing the AI use maybe risky if they don't understand the reaction from evaluators.
The balance of too much transparency and too little transparency is delicate.
Adding more transparency may backlash because evaluators associate the use of AI with lower evaluations.
Having too little transparency may bring concerns about the AI misuse.
A general reaction for organizations to deal with such problem is revealing less information as possible.
However, more and more regulations are coming to regulate the disclosure of AI.
For example, regulators now debate whether AI use must be flagged at the section level (i.e., specifying exactly how or where AI was used) or merely in aggregate (i.e., simply noting that AI was used) (SEC, 2023).

From Baines et al. (2024)
>4.2.1.1.4 Transparency
>
>An additional advisor characteristic that impacts advice utilization by decision-makers is the amount of access that advisors provide to their reasoning and decision process. Specifically, research on human advice has contended that advice discounting may occur partially due to decision-makers’ lack of access to their advisors’ internal justifications and evidence for formulating advice ([Bonaccio and Dalal, 2006](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref10)). Thus, a parallel may be drawn here: a lack of access to and understanding of the underlying computational processes of AI advisors may reduce decision-makers’ likelihood to utilize the AI advice ([Linardatos et al., 2020](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref78)).
>
>Although much research suggests the benefits of transparency of AI advice in terms of the cognitive/affective outcomes of advice (as discussed in a subsequent section), transparency has also been studied with regard to advice utilization (the current focus), with mixed findings. Specifically, some research has found that transparency does not always increase decision-maker advice utilization ([Willford, 2021](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref142); [Lehmann et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref74)). For instance, [Lehmann et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref74) found that the impact of transparency on advice utilization is mediated by the extent to which participants perceive the advice to be valuable, such that participants who interact with a transparently designed algorithm may underestimate its utility (value) if it is simple but accurately estimate its utility if it is complex ([Lehmann et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref74)). [Willford (2021)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref142) also found that participants who interacted with transparent AI relied on it less. This supports the idea that if transparency leads to a lower evaluation of the AI advisor’s utility (i.e., if, once the metaphorical “black box” is opened, what lies inside no longer seems impressive), it does not increase advice utilization. A different explanation proposed by [You et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref145) suggests that the occasionally negative influence of transparency on advice utilization may stem from increased cognitive burden—that is, information provided about AI functioning is complex to the extent that it introduces a detrimentally high cognitive load. Future research should therefore study the circumstances under which AI transparency yields positive versus negative effects—and, in cases involving negative effects, which explanation receives more support.

### Contribution
Our paper provides two features on AI disclosure that are new and important to literature.
First, we recognize that the AI disclosure penalty varies across different types of tasks.
In our setting, we examine the different effects towards important steps in decision making—information, analysis, and conclusion. 
It is new to incorporate different layers of AI disclosure in the same product.
Head-to-head comparison of different trust towards AI in different tasks.
Also, it provides a good examination of interaction effects.
For example, people may like AI collecting information and human doing decisions,
instead of human collecting data and AI doing decisions.
These are two different things.

Second, we examine multiple layers of disclaimer within the same product—analysis report.

The previous research usually view the product as a single entity. Most of the previous literature form the dichotomy of “AI-generated” and “human-generated” product.

Although some research [reference to be added] suggest the AI-human collaboration by indicating “AI-assisted”, it is still unclear at which stage and how the AI is involved.

In our setting, we separately disclose the AI involvement in different tasks of the analysis report.

On one hand, it can show the impact on the different patterns. On the other hand, it can show us the aggregate effect of different combination of the AI involvement to the final decision.

Accountability is a key component in deciding if AI should be involved.

In the market economy, especially in service industry, a non-trivial fraction of price maybe refer to the accountability that the consumer buys the time and effort of a particular expert.

AI involvement may lower the willingness-to-pay of the product.

Under such reaction, producer has incentive to hide their AI involvement.

In our setting, we allow modification towards to choice.

We don’t participants to choose between AI choice and their choice.

We provide a report with AI involvement, but nothing about asking them to choose AI answers.

We mention the report can be erroneous in the disclaimer.

We would like to introduce some 'trusting' preference against the report suggestions.

### Journal article as context
**Sage Journals** (e.g., _Administrative Science Quarterly, Entrepreneurship Theory and Practice, Journal of Management, Journal of Marketing_, and others)

Policy applies to all Sage journals.

Highlights:

- Generative AI content is allowed.
- Use of AI tools must be disclosed clearly and in detail.
- AI tools cannot be listed as an author of a paper.
- AI tools may be used to improve the quality of language in a review of a paper but may not be used to generate review reports, decision letters or summaries of unpublished research.

[Read the full Sage AI policy here](https://www.sagepub.com/journals/publication-ethics-policies/artificial-intelligence-policy).

**Wiley** (e.g., _Contemporary Accounting Research (a journal I happen to be the editor-in-chief of), Econometrica, Journal of Accounting Research_, _Journal of Finance, Journal of Operations Management_, _Strategic Management Journal_, and others)

Policy applies to all Wiley journals.

Highlights:

- Generative AI content is allowed.
- Use of AI tools must be disclosed clearly and in detail.
- AI tools cannot be listed as an author of a paper.
- AI tools may be used on a limited basis for reviewing a paper (e.g., to improve the quality of feedback) but must be declared. This policy varies by journal. For instance, we ban the use of AI at CAR for reviewing.

[Read the full Wiley AI policy here](https://authorservices.wiley.com/ethics-guidelines/index.html#22).

**Academy of Management**

Policy applies to all AOM publications.

Highlights:

- AI tools cannot be listed as an author of a paper.
- AI tools cannot be used as a resource in reviewing a paper.
- Use of AI tools are only allowed to support the following:

- Spelling
- Grammar
- Data Collection/Analysis

[Read the full AOM AI policy here](https://www.aom.org/publications/journals/publishing-with-aom/aom-artificial-intelligence-policy/).

If you have any questions, please contact Joanne Pereira and her team.

Partha

# Study 1 method
I first examined how disclosing AI use affected trust in the investment recommendation of a financial report. Participants read the same professional financial report but were shown different disclosures about AI use. Participants will be randomly assigned to one of nine experimental conditions. Eight of these conditions consist of a 2 (Information Gathering: AI vs. No AI) x 2 (Analysis: AI vs. No AI) x 2 (Recommendation: AI vs. No AI) between-subjects factorial design. The ninth condition, included for exploratory purposes, is a 'No Disclosure' group, where I remove the "Generative AI Disclosure" section in the investment report. Participants were randomly assigned to one of the nine conditions.
### Benefit of using experimental method
One key problem of examining the effect of AI disclosure is we are not able to completely separate the effects from product quality and disclosure itself. 


The report compares and formulates recommendation to invest in one of the three companies. The report was claimed to be prepared by Verdian Equity Research, which is a fictitious company we named. The companies they will invest in are real listed companies.The report has three sections: Information, Analysis, and Recommendation. The Information provides fundamental business overview and the company financials of all three companies. The analysis part provides analysis on the business trends and explanations of the recent trend among the three companies. The recommendation part provides explicit recommendation to one of the three companies with reasons and cautions.  

### Procedure
The study consists of three sections. The first section presents an investment report that provides information and recommendation among three companies. Participants were asked to invest in one of the three companies that they believe to be highest returned in the stock market. The second section included questions that explore the mechanisms that aim to understand participants' decision making process. The last section collected information on age, gender, occupation, income level, prior investment experience, and self-reported usage of AI tools. 
The data collection period was September 18, 2025 to September 20, 2025. Participants who are registered and qualified with the filters in the study will be notified by Prolific. Participants can choose to enter the Qualtrics study link on Prolific task recruitment. After entering the study, participants read a consent about the study purpose, who are the researcher, benefits, terms for privacy. For the study purpose, participants were told that it is a study about "investment decision making". Participants can agree to enter the study by entering their Prolific ID. After the consent page, participants read an introduction about the study overview. The stakes are real and they will get monetary bonus if they choose the highest returned company among three companies over the six-month period. The period of calculating six-month returns was from March 30, 2025 to September 30, 2025. The end date was set after the end of the study collection on purpose. Participants were also told that the companies are real and their identities will be revealed at the end of the study. After reading the report, participants are asked to make an investment decision to choose one of the three companies to invest in. If participants choose the highest returned company in the last six month by the end of the period in the stock market, they will get a monetary bonus. The key independent variable is whether participants choose to invest in the recommended company in the report. After making the main investment decisions, participants were asked about mechanism questions and demographic questions. When participants finish all the questions, I present the debrief about the deception about study purpose and explain why I need to use the deception. I also reveal the real names of the companies. 

### Financial reports
I use financial report in our setting that has merits and can be externally valid; it is not a paper for finance journals but for organization/decision making.
We use the setting of financial investment by choosing one of the three companies which has highest stock market return. I used the financial investment setting for several reasons. 
1) Financial setting is a scenario where rational decision making is imperative. The aesthetic or objective preference is tampered in financial investment scenario. Participants who aim to maximize their monetary return need to judge the information with caution, our manipulation will be more salient. Compared to industries like art or essay evaluation, the financial scenario is closer to what we want: participants use all the information judiciously and make their decisions. 
2) Financial setting is easier to motivate the incentive. It feels natural to get bonus when they choose correctly the highest amount. It is easy to find objectively measured performance in stock market because the stock price is public. It creates an objective measured options for participants to use the information to make best decision.
3) Financial setting is easier to implement our study goal of multiple roles in a task. It is natural and necessary to have gathering information, conducting analysis, and providing recommendation. Financial report is a complicated and it involved different thinking levels.
4) It is essentially an "information industry". The report per se has no meaning unless people adopt the knowledge and recommendation in it. The setting is relevant to our study purpose: we want to have a inherently hard to predict scenario, where we rely on experts on the market to consolidate information, and we rely on the consolidated information to make decisions.

Some examples of AI preference and Financial context:
Using the financial context is commonly used in the literature.
[[@yangMyAdvisorHer2025]] uses the alignment of final decision with advice under AI or Human to measure.
Merkle (2025) uses the rate at which participants follow the recommendation either from ChatGPT or from a financial professional.
Stradi and Verdikt (2025) uses how investors are less responsive to forecasts when AI is incorporated.

[[@caoManVsMachine2024]] tests the ability of AI stock analyst versus human analyst. The study shows that the AI investment advisor is gaining attention in academic setting.

### Dependent variable
The dependent variable is the proportion of participants making investment decision the same as the recommendation. If the Generative AI Disclosure is not affecting the decision making in the investment, all groups should have the same proportion of investing the companies recommended in the report. The underlying assumption is participants partly use the report's recommendation as guideline, and they choose whether to believe in the investment decision or not. It does not imply that all participants use the recommendation in the report to make their investment (totally possible that people may choose to make their own decision), and it does not imply that imply that people only make their investment based on whether trust or not trust the report recommendation. The report recommendation is a strong message provided to participants, consciously or even unconsciously affecting their investment decision. The investment decision of whether following the investment recommendation has good features in understanding how people make decisions with complicated information. We are not simply ask people do they trust one piece of information, but we ask them to do a staked investment decision with the information we provided. How are they going to pick up the small cue of Generative AI Disclosure is the key to our study. Compared to other study, our study design has the merit of understanding how a small piece of information may change the "big" decision making, and how this small piece of information is going to change how people evaluate other provided information. This is important both academically and realistic in the actual decision making scenarios.

### How I prepared the reports
To reduce the the template effects (potential specificity in one report with specific features that drives the results), I have prepared four reports. 

The first two of the four reports were prepared by me with the help of Generative AI tool Gemini 2.5 Pro. 
Why I used Generative AI to help creating the reports? First, I want to at least have some parts of Generative AI involved because the reports should be not obvious for participants to tell if they are AI or human generated. Second, it is difficult for us to find existing equity research reports for our study purpose. We want to the reports to have relatively clear divisions of  gathering information, conducting analysis, and formalizing recommendation. Also, to simplify our study, we want the report to recommend one of three companies instead of generic recommendations. The specific requirements are difficult to achieve with existing real-world reports. 
In preparing the reports, the major considerations are the industries and companies. 
For the industries, the requirements should be neutral and not evoke excessive personal moral preference. This is because we want our participants use more weight from the reports instead of other factors such as personal tastes or moral tastes.
- Neutrality and Low Controversy. The goal of the financial report is to provide financial advice to people who are willing to invest in. For example, the firearm,  I would like to avoid the situation where people are not willing to invest in even before reading the reports.
- General understanding capacity. In addition to the neutrality, I would also like to have the companies that are understandable to an extent. The understandability should be enough for participants to understand the business model and how they make profits. A counter example could be a highly specialized financial company using complicated financial derivatives to make profit.
- Third, I also don't want the industry to be the industry that people use in their daily life. The prior familiarity may cause prior preference without the reports in the decision making process.

After reviewing different industries that are aligned with the requirements above, I choose the  Medical Devices and HR Software industry to create the investment report. These two industries are aligned with the requirements. In addition, the two industries also introduce a variance in both manufacturing industry and software industry. 

I first use Gemini 2.5 Pro Deep Research to create a report for medical industry and HR software separately. I manually verified the information in the report and made changes in the report.

After preparing the one report for both Medical Device and HR Software, I proceed with creating two extra reports. To reduce the obvious AI cues and enhance the professional writing in the report, I ask professional writing firm to edit the reports. The four reports are not study conditions, and I pooled them into one of the study condition without making a distinction between them. I have conducted pre-testing to examine the quality across the report, and I did not find evidence that any of the reports are driving result patterns significantly.

### Information, Analysis, and Recommendation
I divide the report into three parts: information, analysis, and recommendation.
I do it because 1) it is a common way to make decision structurally, and 2) it helps to clearly separate the part when I present the Generative AI Disclosure. 
The structure of 
The information section shows the fundamentals of business and the financial information. The analysis goes deep into the patterns of the information and analyze why pattern happen (e.g. why the revenue drops, why they decide to invest large in certain business). The recommendation gives final recommendation of one of the three companies. It is giving investor a clear recommendation why they should buy one of the three companies instead of the other two. It is okay to put in caution and analyze the bull/bear forecast. But still, here it is less about the analyzing but about recommendation. 


To ensure that people are making decisions based on the report materials, I have conducted two measures to prevent participants to look up the stock price by themselves.
First, I use pseudonyms for all companies in our study. In the introduction section, I mention that all companies in the study are real, and participants will be informed about the real companies behind each pseudonym after the study. In addition, I also prevent participants from copying text in the survey. Disabling the copying can reduce the probability that participants use AI tools to guess the real names of the companies. 

I set the end date of calculating the stock returns to be 10 days after the launch of the study. This aims to preserve the prediction feature of the report. It may cause difficulty and confusion if participants were asked to predict the price in the past. In addition, it also brings extra benefits of showing researchers are not cherry picking about the existing stock to fulfill the stock patterns.

Which report should be recommended in the report? To avoid the bias of always recommending the best company or not recommending the best one,  I have prepared two situations that recommend both the actual highest return company and the other one does not. In the HR software industry, the recommended firm is different from the highest returned firm. The recommended firm is Kforce (KFRC), and the highest returned firm is Heidrick & Struggles (HSII). In the Ophthalmic Medical Device report, the recommended firm is the same as the highest return firm. The recommended and highest-returned firm is EyePoint Pharmaceuticals (EYPT). 

 The main goal of the report is 1) not obvious to audience that it is AI generated and 2) has variation in final selection. 
 I have conducted a pre-testing experiment (N = 100) to show that people are able to tell to what extent the reports are generated by AI. It is consistent with the studies showing that it is difficult to tell AI-generated text and human writing (Waltzer et al., 2023; Gunser et al., 2021). The pre-testing results show that a majority of people think the texts are equally likely to be written by human or AI. 
I have also conducted a pilot study (N = 40) to examine whether people will follow the report recommendation unanimously. One concern in the study is that participants may overlook the information and just follow the recommendation. The lack of decision variance may cause problem in low statistical power. In our pilot study, we do see that people are choosing companies other than the recommended. In addition, we also want to see if people are aware of the study purpose, which may cause the demand effect and bias the answer. We did not find any evidence about the highly concentrated choices in choosing the companies.
### Participants
I recruited participants from Prolific. To filter the participants best suited to the study purpose, I apply the following filters to the selection of potential participants: FILTER1, FILTER2... 
The study recruited 1,350 participants, with approximately 150 in each of the nine conditions. After the recruitment, I excluded four observations with duplicated IDs, thus ended up with 1,346 observations in our final sample. The slight difference in the number of each group and proposed 150 is because some participants entered but left the experiment during their answers, thus causing some small differences in random assignment to groups. 
### Template effects
To make sure the effects are not driven by particular features in the gender of report author, I assign the report author name to either the male, female, or neutral names. The names used were Christopher Davis, Sarah Brown, and J.M. Wright. 

### Disclosure Manipulation
At the end of the report, I included a "Generative AI disclosure" section. In the section, I 
Participants will be randomly assigned to one of nine experimental conditions. Eight of these conditions consist of a 2 (Information Gathering: AI vs. No AI) x 2 (Analysis: AI vs. No AI) x 2 (Recommendation: AI vs. No AI) between-subjects factorial design. The ninth condition, included for exploratory purposes, is a 'No Disclosure' group, where I remove the "Generative AI Disclosure" section in the investment report. Each participant will only be exposed to one of the nine conditions.

We included the "No" because 1) there are real use of them in the industry to separate themselves with the others, and 2) [[@longDisclosureDilemmaHow2025]] shows there is no difference between the two. It is interesting that we found opposite results with them.

# Pilot study in Appendix
We implemented our pilot study with X participants. Compared to the actual survey, we implemented extra pre-testing questions.
There are two sets of pre-testing questions. One set asks questions about the quality of the reports, and the other set asks the participants to guess the study purpose. The former is necessary because we want to 
# Study 1 Results
I assess the main research question by examining the percentage of investing report's recommended company group by manipulation condition. In the study, I assigned about 150 participants to each group, which constitutes about 1,350 participants in the study. 
### Disclosure Penalty 
The results are presented in Figure 1. I found that the groups with AI disclosure are on average 13 percentage points lower than the group without AI disclosure in following the recommendation. It shows that if we randomly assign contents in the AI disclosure, people are on average less likely to follow the recommendation given in the financial report. The difference between the disclosure and non-disclosure groups are significant (T = -3.127, p = 0.002). 
### Fully-AI disclosed report
The groups with AI disclosure contains eight different manipulation conditions. What about these conditions? 
Figure 2 shows the percentage of followed recommendation grouped by manipulation conditions of AI disclosure.
I found that most of the groups exhibit lower percentage of following advice. The only exception is the Yes-Yes-Yes, which the report claims to involve AI in all three sections: gathering information, conducting analysis, and formulating recommendation. There is no significant difference across disclosures with no-group and partial use groups. However, there is significant difference between the Yes-Yes-Yes group with all other disclosure conditions. The percentage of following recommendation in group disclosing no AI use in all sections (No-No-No) is 14 percentage points lower than Yes-Yes-Yes group (T = -2.48, p = 0.01). 

In sum, the content of disclosure matters. 1) The Fully disclosed AI use disclosure has significant more higher percentage in following the recommendation than all other groups and 2) The No use and Partial use groups are not significantly different from each other.

In Study 1, I have tested mechanisms that may undermine the perceived quality of the report. There is no evidence that the perceived efforts, perceived originality, perceived accountability, perceived transparency, and perceived reliability in the report drive the patter in revealed trust towards the report. 

# Study 2 Results
In Study 2, I want to replicate the Study 1 results and explore the underlying mechanisms driving the patterns.
In Study 2, I tested three mechanisms that may drive the patterns in the Study 1. First, I test whether participants shift their attention to the process of preparing the investment report. Second, I test whether participants are skeptical about the disclosure is truthful. Third, I test whether the participants exhibit different evaluation towards the perceived expertise, benevolence, and integrity.

### Participants. 
In Study 2, the participants were again recruited on Prolific. I filter the participants as the same in the Study 1. The study included 1200 participants. 

### Mechanism 1

### Mechanism 2
### Mechanism 3

Mayer and Davis (1999) adapted. We also see Schilke and Reimann (2025) adpated the scales to measure the trustworthiness of the financial advisor.


# Conclusion
