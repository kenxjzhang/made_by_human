# Introduction
Evaluation represents a comprehensive understanding to related characteristics of the producer.

One key advantage of human product is the reputation to be observed.

Reputation shows two sides: looking backwards and predicting future.

People have recognized that we have different opinions towards product that are made by human and made by algorithm or AI.

There is different pattern when it comes to follow the advice given by human and AI.

The dichotomy of convention judgement about human product or AI product seems to be reasonable.

However, we do see that AI therapist is getting popular and it was underestimated when the Generative AI first came up.

If we follow the logic of ‘emotion’ versus ‘rational’ choice different, we should expect them to be wary about the use of AI in giving them advice on their personal relationship and the deep emotions.

The standing ground here is that people judge AI-generated product beyond the simple quality judgement, but foreseeing the consequences it may caused.

People may devalue the product because it is simply ‘too fast’.

Human beings are prone to use effort heuristics, which links effort with quality of the product.

Even if the quality seems the same, people may think about the consequences in the future.

There is a straightforward calling to solve the confusion along with the AI in the work: disclosing the use of AI.

The key audience that I try to focus is the AI transparency literature, which is how and what is the consequence of disclosing the AI use in the work.

In our study, we deliberately choose the structural decision making as our main goal needs AI assist.

While there are more callings about AI product disclaimer, we have little evidence about what will be look like if we are providing a long, structural report.

The report structure is coherent with the McKinsey’s famous SCR method of consulting: Situation, Complication, and Resolution.

[An argument is why I am using the structural report to analyze it.]

Is it commonly used in the business world? Is it fundamental to decision making? ]

Similar to [[@stradiAlgorithmAversionAppreciation2025|(Stradi et al., 2025)]], I include “Human+Machine”, in addition to “Human” and “Machine”.

Disclosing the AI usage may not be favored by producer, because they may lose trust from evaluators. [[@schilkeTransparencyDilemmaHow2025|(Schilke et al., 2025)]]

### Human-AI collaboration is the trend
Ethan Mollick's book Co-intelligence points out the “jagged frontier” of AI tools. To complete complicated tasks, it is necessary to have human input in some areas of weakness in AI. 
More and more research discuss about the Human-AI collaboration increases the productivity.
However, our research try to show that the productivity, as somewhat measured objectively, also missed the discussion on how people perceived the AI-assisted products. In lots of industries, 
Asktitas (2025) shows that human-AI collaboration is prevalent in academic writing. 
More research is on the topic. 
Cathy et al. (2025, MS forthcoming) talks about the [[Emails from Andras#2025-08-18 09 13]]
[[@caoManVsMachine2024]] uses AI-human financial analyst and compare the productivity between the two.




### AI label effects have been tested in...
>It is welldocumented that new technologies are often met with skepticism. Studies suggest a general sentiment of hesitancy (e.g., von Eschenbach, 2021) and mild to moderate aversion (e.g., Castelo & Ward, 2021; Jussupow et al., 2020) towards AI and computer algorithms more broadly. Also, when told that AI was involved in the creation of communicative content, there was some reporting of preference against or lower evaluation of that content (e.g., Airbnb profile writing; Jakesch et al., 2019; email writing; Liu et al., 2022; generated paintings; Ragot et al., 2020; music creation; Shank et al., 2023; translation of written content; Asscher & Glikson, 2023). Within health contexts especially, some studies show that people tend to prefer human practitioners over AI-based technologies like chatbots when receiving consultation about health conditions (e.g., Miles et al., 2021), citing lack of personalization and incompetence in addressing individual needs as some of the reasons for hesitancy (Longoni et al., 2019).
>For example, Ragot et al. (2020) found that when people perceived AI as the creator of artwork, they evaluated the art more negatively (compared to human-generated artwork) in terms of beauty, novelty, or meaningfulness. Specifically in the context of health messaging, Karinshak et al. (2023) examined how source disclosure impact people’s ratings of health campaigns messages. **(From Lim and Schmälzle, 2025) (Don't plagarize.)**

>In terms of decision-maker preferences between human and AI advisors, some research has found that decision-makers prefer human over AI advice (e.g., [Dietvorst et al., 2015](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref25); [Larkin et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref72)). This is one form of what is often referred to as algorithm aversion, or general negative attitudes and behaviors toward the algorithm ([Logg et al., 2019](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref81); [Lai et al., 2021](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref69)). For example, [Larkin et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref72) found that participants indicated they would prefer to receive recommendations from a human expert versus AI in financial and, even more so, healthcare and contexts. Similarly, [Dietvorst et al. (2015)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref25) found that, across forecasting tasks on student performance and airline performance, decision-makers consistently chose human judgment when choosing between AI forecasts and either their own forecasts or the forecasts of another human participant.
>
>However, other research has found the converse (e.g., [Logg et al., 2019](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref81); [Kennedy et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref62)). For example, [Kennedy et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref62) found that in geopolitical and criminal justice forecasting experiments, decision makers placed a higher weight on AI advice (i.e., forecasting algorithms) relative to several kinds of human advice (i.e., aggregate of expert decision-maker responses; aggregate of non-expert decision-maker responses)–in other words, algorithm appreciation rather than aversion.

### Credibility
Credibility is both a systematic and heuristic evaluation.

Metzger et al. (2010) shows that the credibility is mostly through heuristic/non-deliberate way
>Results also indicate that rather than systematically processing information, participants routinely invoked cognitive heuristics to evaluate the credibility of information and sources online.

### Dilemma of transparency and disclosure
[[@longDisclosureDilemmaHow2025]] found that  message will lose credibility after revealing the AI-label
Our work is different because we only say the AI 'involved'
[[@toffTheyCouldJust2024]]
[[@schilkeTransparencyDilemmaHow2025|Schilke et al. (2025)]]




For these papers, they think about the AI label on message credibility, but they haven't looked into the credibility of the disclosure itself.



### Contamination effects of AI transparency
For producers, revealing the AI use maybe risky if they don't understand the reaction from evaluators.
The balance of too much transparency and too little transparency is delicate.
Adding more transparency may backlash because evaluators associate the use of AI with lower evaluations.
Having too little transparency may bring concerns about the AI misuse.
A general reaction for organizations to deal with such problem is revealing less information as possible.
However, more and more regulations are coming to regulate the disclosure of AI.
For example, regulators now debate whether AI use must be flagged at the section level (i.e., specifying exactly how or where AI was used) or merely in aggregate (i.e., simply noting that AI was used) (SEC, 2023).

From Baines et al. (2024)
>4.2.1.1.4 Transparency
>
>An additional advisor characteristic that impacts advice utilization by decision-makers is the amount of access that advisors provide to their reasoning and decision process. Specifically, research on human advice has contended that advice discounting may occur partially due to decision-makers’ lack of access to their advisors’ internal justifications and evidence for formulating advice ([Bonaccio and Dalal, 2006](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref10)). Thus, a parallel may be drawn here: a lack of access to and understanding of the underlying computational processes of AI advisors may reduce decision-makers’ likelihood to utilize the AI advice ([Linardatos et al., 2020](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref78)).
>
>Although much research suggests the benefits of transparency of AI advice in terms of the cognitive/affective outcomes of advice (as discussed in a subsequent section), transparency has also been studied with regard to advice utilization (the current focus), with mixed findings. Specifically, some research has found that transparency does not always increase decision-maker advice utilization ([Willford, 2021](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref142); [Lehmann et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref74)). For instance, [Lehmann et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref74) found that the impact of transparency on advice utilization is mediated by the extent to which participants perceive the advice to be valuable, such that participants who interact with a transparently designed algorithm may underestimate its utility (value) if it is simple but accurately estimate its utility if it is complex ([Lehmann et al., 2022](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref74)). [Willford (2021)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref142) also found that participants who interacted with transparent AI relied on it less. This supports the idea that if transparency leads to a lower evaluation of the AI advisor’s utility (i.e., if, once the metaphorical “black box” is opened, what lies inside no longer seems impressive), it does not increase advice utilization. A different explanation proposed by [You et al. (2022)](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1390182/full#ref145) suggests that the occasionally negative influence of transparency on advice utilization may stem from increased cognitive burden—that is, information provided about AI functioning is complex to the extent that it introduces a detrimentally high cognitive load. Future research should therefore study the circumstances under which AI transparency yields positive versus negative effects—and, in cases involving negative effects, which explanation receives more support.

### Contribution
Our paper provides two features on AI disclosure that are new and important to literature.
First, we recognize that the AI disclosure penalty varies across different types of tasks.
In our setting, we examine the different effects towards important steps in decision making—information, analysis, and conclusion. 
It is new to incorporate different layers of AI disclosure in the same product.
Head-to-head comparison of different trust towards AI in different tasks.
Also, it provides a good examination of interaction effects.
For example, people may like AI collecting information and human doing decisions,
instead of human collecting data and AI doing decisions.
These are two different things.

Second, we examine multiple layers of disclaimer within the same product—analysis report.

The previous research usually view the product as a single entity. Most of the previous literature form the dichotomy of “AI-generated” and “human-generated” product.

Although some research [reference to be added] suggest the AI-human collaboration by indicating “AI-assisted”, it is still unclear at which stage and how the AI is involved.

In our setting, we separately disclose the AI involvement in different tasks of the analysis report.

On one hand, it can show the impact on the different patterns. On the other hand, it can show us the aggregate effect of different combination of the AI involvement to the final decision.

Accountability is a key component in deciding if AI should be involved.

In the market economy, especially in service industry, a non-trivial fraction of price maybe refer to the accountability that the consumer buys the time and effort of a particular expert.

AI involvement may lower the willingness-to-pay of the product.

Under such reaction, producer has incentive to hide their AI involvement.

In our setting, we allow modification towards to choice.

We don’t participants to choose between AI choice and their choice.

We provide a report with AI involvement, but nothing about asking them to choose AI answers.

We mention the report can be erroneous in the disclaimer.

We would like to introduce some 'trusting' preference against the report suggestions.

# Study method
### Financial report and investment advice
In this section, I am going to argue that the use of financial report in our setting has merits and can be externally valid; it is not a paper for finance journals but for organization/decision making.
We use the setting of financial investment by choosing one of the three companies which has highest stock market return. 
1) Financial setting is a scenario where rational decision making is imperative. 
2) Financial setting is easier to motivate the incentive. It feels natural to get bonus when they choose correctly the highest amount. It is easy to find objectively measured performance in stock market because the stock price is public. It creates an objective measured options for participants to use the information to make best decision.
3) Financial setting is easier to implement our study goal of multiple roles in a task. It is natural and necessary to have gathering information, conducting analysis, and providing recommendation. 
4) It is essentially an "information industry". The report per se has no meaning unless people adopt the knowledge and recommendation in it. The setting is relevant to our study purpose: we want to have a inherently hard to predict scenario, where we rely on experts on the market to consolidate information, and we rely on the consolidated information to make decisions.

Measuring the advice following is important and is commonly used in the literature. [[@yangMyAdvisorHer2025]]] uses the alignment of final decision with advice under AI or Human to measure.


### Reports
I prepared 4 reports.

The first two of the four reports were prepared by me with the help of Generative AI (Gemini 2.5 Pro). 
Why I used Generative AI to help creating the reports? First, I want to at least have some parts of Generative AI involved because the reports should be not obvious for participants to tell if they are AI or human generated. Second, it is difficult for us to find existing equity research reports for our study purpose. We want to the reports to have relatively clear divisions of  gathering information, conducting analysis, and formalizing recommendation. Also, to simplify our study, we want the report to recommend one of three companies instead of generic recommendations. The specific requirements are difficult to achieve with existing real-world reports. 
In preparing the reports, the major considerations are the industries and companies. 
For the industries, the requirements should be neutral and not evoke excessive personal moral preference. This is because we want our participants use more weight from the reports instead of other factors such as personal tastes or moral tastes.
- Neutrality and Low Controversy. The goal of the financial report is to provide financial advice to people who are willing to invest in. For example, the firearm,  I would like to avoid the situation where people are not willing to invest in even before reading the reports.
- General understanding capacity. In addition to the neutrality, I would also like to have the companies that are understandable to an extent. The understandability should be enough for participants to understand the business model and how they make profits. A counter example could be a highly specialized financial company using complicated financial derivatives to make profit.
- Third, I also don't want the industry to be the industry that people use in their daily life. The prior familiarity may cause prior preference without the reports in the decision making process.

After reviewing different industries that are aligned with the requirements above, I choose the Ophthalmic Medical Devices and HR Software industry. These two industries are aligned with the requirements. In addition, the two industries also introduce a variance in both manufacturing industry and software industry. 


After preparing the first 




### Report manipulation
Participants will be randomly assigned to one of nine experimental conditions. Eight of these conditions consist of a 2 (Information Gathering: AI vs. No AI) x 2 (Analysis: AI vs. No AI) x 2 (Recommendation: AI vs. No AI) between-subjects factorial design. The ninth condition, included for exploratory purposes, is a 'No Disclosure' group, where I remove the "Generative AI Disclosure" section in the investment report. Each participant will only be exposed to one of the nine conditions.
**

We included the "No" because 1) there are real use of them in the industry to separate themselves with the others, and 2) [[@longDisclosureDilemmaHow2025]] shows there is no difference between the two. It is interesting that we found opposite results with them.

# Pilot study
We implemented our pilot study with X participants. Compared to the actual survey, we implemented extra pre-testing questions.
There are two sets of pre-testing questions. One set asks questions about the quality of the reports, and the other set asks the participants to guess the study purpose. The former is necessary because we want to 
# Experiment #1


# Experiment #2


Meyer (1988) suggests a credibility model based on two factors: believability and community concern. We test the believability in the study and adapt a single-item question to ask to what extent do participants believe the Generative AI disclosure is a truthful representation of the actual AI use in report.




### Survey study
### Cautionary actions
We use pseudonyms in our study because we want our participants to use the report to make their decisions in choosing the highest return company. I also disabled the copy and paste in the survey to minimize the probability that participants use AI tools to guess the real names of the companies.


# Prepare the reports
### Writing-reports
In the study, 
### Financial report as an example
We decide to use financial investment decision as our context.
The context is chosen because it represents the 'rational' side of decisions.
Choosing the highest return stock does not involve too much of the personal tastes such as art evaluation.

The decision behind an investment involves different layers of information and suggestions.
(_Reference?_) to show that it is a complicated decision to make.
Investment decision is an example, not our intention to test the theory.
Our main goal is not to understand how AI involvement would increase or decrease the quality of the report.
Rather, our goal is to understand the acceptance of evaluators towards the product.
The financial report suggestion is one of the examples of report that aids decisions for other people.
Think about a performance report of an employee written by group leader to head manager.
The performance report should be used to help head manager to decide if she wants to promote the person.
Under the circumstance, the involvement of AI in the writing may change the decision of the head manager.

As a result, what are the key ingredients for our report in the study?
- Transportable (comparable to other scenarios)
- Informative (to help them make decisions on the stocks)

### How to make sure that we are doing okay with these two points?
A simple idea is make the report less financial savvy.
No financial shorthands should be included (ROI, P/E). If included, I would also provide explanation to what it means.
Make it more like a financial analyst who are trying to explain the situation.
There are a million way to recommend stocks to an investor.
We are interest in how to make the recommend to be a little universal, but not too much.
Easier said than done. 
What should we actually compile the reports like these?
Pro:
- Universal nature, easier to justify the result
Con:
- People may get suspicious
	- though less likely, and we can always put it in the pre-testing
Okay let's try to work on it.
Should we put it into Gemini and ask it first?
Gemini's answer
>- **信息 (Information) 的作用：建立一个共同的现实基础** 它的核心任务是回答一个问题：“**我们在谈论什么？**” 这部分不是为了展示资料搜集得有多全，而是为了将您和读者拉到同一个“房间”，确保你们看着同样的事物。在评价任何东西之前，我们必须先对它有一个共同的、无争议的认知。信息部分就是这个“**定场**”的环节，它扫清了模糊地带，为后续的一切讨论提供了稳固的地基。如果这个基础不牢，后面的分析和推荐就会像建在沙滩上的城堡。
>- **分析 (Analysis) 的作用：提供一个客观的比较框架** 它的核心任务是回答一个问题：“**它们之间有何不同，优劣何在？**” 这是整个报告的逻辑核心。它的作用不是为了说服，而是为了**展示**。您在这里把三家公司放到同一个天平上，从相同的几个维度（如增长、盈利、优势）去称量它们。这个过程必须是**中立和透明的**，您要像一位侦探，把所有的证据，无论是正面的还是反面的，都一一呈现给读者看。这个部分的作用是建立您的**可信度**，因为它向读者展示了您的思考过程，读者在这里跟随您一起审视证据，而不是被动地听取结论。
>-  **推荐 (Recommendation) 的作用：做出一个负责任的最终决断** 它的核心任务是回答最终的问题：“**所以，我们应该怎么选？**” 这是整个旅程的终点。它的作用不是重复分析中的证据，而是基于那些证据，做出一个**综合性的、有倾向性的判断**。如果说分析是“看清所有路径”，那么推荐就是“指出最好走的那一条”。这个部分是**勇气和智慧**的体现，它将前面客观的、发散的比较，收敛成一个单一的、明确的行动指令。它赋予了整篇报告一个最终的意义和价值——帮助读者做出决策。

Information collection - Argument - Decision
Great -- here is my overall idea. 
These three parts should be relatively easy to read.

