**Explanations of the five mechanisms:**

1. **Effort Heuristics**  
   * This mechanism posits that individuals use the perceived effort invested in producing a product or judgment as a mental shortcut (a heuristic) to gauge its underlying quality and value.  
   * In the absence of perfect information, we often assume a positive correlation between effort and quality. A report that is perceived to have taken significant time, cognitive energy, and expertise to create is seen as more likely to be thorough, vetted, and valuable. Generative AI disrupts this heuristic by creating an impression of "effortlessness." Content generated in seconds by an AI, even if high-quality, may be devalued because it lacks the costly signal of human labor, leading users to question its credibility and worth. Even if the product is not completely generated by AI, people perceive it with lower quality since it takes less time to complete.  
2. **Novelty Deficit**  
   * This mechanism suggests that individuals perceive AI-generated insights as inherently less novel and more conventional than those produced by human experts.  
   * Humans are often credited with the capacity for true "generative" thought—creating genuinely new ideas and unconventional connections. In contrast, generative AI is often perceived, correctly, as a highly sophisticated pattern-matching and "integrative" engine that synthesizes existing information from its training data. This leads to a belief that AI's output, while fluent and comprehensive, represents a form of "average wisdom" and is less likely to contain the unique, paradigm-shifting insights that are highly valued in strategic and financial analysis.  
3. **Impaired Accountability**  
   * This mechanism proposes that human-AI collaboration creates an ambiguous joint-agency, which diffuses responsibility and makes it difficult for observers to assign clear ownership for the output's quality and consequences.  
   * Accountability relies on the presence of an identifiable agent who can be held responsible. When a human works alone, they "own" the output. When an AI works alone, its owner/developer can be held responsible. However, a human-AI collaboration creates an "accountability vacuum." Errors can be blamed on the "black box" AI by the human, while the AI itself has no social or legal agency to bear responsibility. This ambiguity and lack of clear ownership can be unsettling for users, especially in high-stakes domains like finance where recourse for bad advice is critical.  
4. **Black-box Discomfort**  
   * This mechanism suggests that individuals feel a "black box discomfort" stemming from their inability to understand the underlying logic and reasoning process of an AI, even when its output appears coherent.  
   * Transparency is the perceived ability to "see through" a process and understand how inputs lead to outputs. Human reasoning, while complex, is assumed to follow familiar patterns of logic, experience, and deduction. In contrast, the process of a large language model—predicting the next word based on complex statistical patterns in vast training data—is fundamentally opaque and counterintuitive to users. This disconnect between the fluent, human-like output and the alien, statistical process can reduce trust, as users may feel the report's conclusions are not grounded in genuine reasoning but are merely an artifact of a process they cannot scrutinize.  
5. **Hallucination Risk**  
   * This mechanism addresses the specific concern that generative AI, unlike other forms of automation, is prone to "hallucination"—the confident assertion of plausible-sounding but factually incorrect or entirely fabricated information.  
   * While any report can contain errors, the concept of hallucination is unique to modern generative AI. It is not simply an error in calculation or a typo; it is the creation of non-existent facts, sources, or citations. The awareness of this specific type of risk—that the AI might invent a "fact" that perfectly supports its argument—introduces a novel layer of skepticism. Users may fear that they cannot trust the very building blocks of the report's argument to be tethered to reality.