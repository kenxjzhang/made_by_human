---
tags:
  - AI
  - writing
---
## [[22 New Jobs A.I. Could Give You - The New York Times]]
Trust
> In a sense, both of Seamans’s visions fall into a broader category of “trust.” I didn’t submit my A.I.-generated article in part because that would have betrayed my editors’ trust, but also because I didn’t trust _it_ — trust that it was true, trust that it got the facts right. Because I hadn’t done the work and the thinking myself, I couldn’t tell if it was being fair or reasonable. Everyone who tries to use A.I. professionally will face a version of this problem: The technology can provide astonishing amounts of output in an instant, but how much are we supposed to trust what it’s giving us? And how can we know?

Accountability

> At its core, trust is about accountability — and this is where a human in the loop is most critical. In everything from contracts to nuclear-launch systems, we need humans to be accountable. “There should be a human who ultimately takes responsibility,” said Erik Brynjolfsson, director of the digital economy lab at the Stanford Institute for Human-Centered Artificial Intelligence and also a founder of the A.I. consulting company Workhelix. “Right now if a car crashes, you have to sort out: Is it the antilock brakes? Was it the driver? Was there something wrong in the road? If it’s the antilock brakes, who was it who made that part? And they trace it back to who ultimately is responsible for that thing. It may be a complex chain of causality, and it’s going to get that much more complicated with A.I., but ultimately you have to trace it back to somebody who takes responsibility.”

Consistency

> Another new role will be some type of **consistency coordinator**. A.I. is good at many things, but being consistent isn’t one of them. Can a fashion house be assured that a particular dress will be accurate and consistently represented across dozens of A.I.-generated photographs? In manufacturing, can a virtual twin manager — someone who manages and tweaks software versions of real-world objects and systems — be sure that A.I.-made digital replicas will stay consistent as new changes are implemented? And when A.I. isn’t consistent, it can’t be trusted. This is where a dedicated role, one that can accept accountability, will be needed to validate consistency across systems and organizations.

Taste

> After a bit more back-and-forth, Cooper asks, “So what are you being paid for?”
> 
> Rubin answers the question: **“The confidence I have in my taste, and my ability to express what I feel, has proven helpful for artists.”**

Writing before AI writes

> There’s plenty of reason to lament the loss of craft, of course. It’s grim to imagine an age when our writers don’t write, our musicians don’t play instruments and our illustrators don’t draw. But that’s not really the age we’re entering; the act of craft, after all, will always have a huge impact on thinking. Mollick sees this even when it comes to his academic writing. “I will have it do research in advance, but I will never let it write before I write,” Mollick said of A.I. “I have to write messily to think something through. Otherwise, the A.I. will dominate my thoughts.”


## [[AI’s Trust Problem]]
There are 12 different concerns were talked about in the article. I am more interest in the following: Disinformation, The Black Box Problem, Instability, Hallucination, Unknown unknowns.

> 考虑以下 12 种在两组人中都被广泛引用的人工智能风险：
> - Disinformation  虚假信息
	- Safety and security  安全和安保
	- The black box problem  
    黑盒问题
	- Ethical concerns  伦理担忧
	- Bias  偏见
	- Instability  不稳定性
	- Hallucinations in LLMs  LLMs 中的幻觉
	- Unknown unknowns  未知之未知
	- Job loss and social inequalities  
	    工作失业和社会不平等
	- Environmental impact  环境影响
	- Industry concentration  行业集中度
	- State overreach  政府过度干预

Watermarking or clear labeling of the AI generated contents
>In the U.S., multiple states [have introduced](https://archive.is/o/yFhaw/https://www.citizen.org/article/tracker-legislation-on-deepfakes-in-elections/) bills targeting elections-related disinformation and deepfakes. The White House has an [executive order](https://archive.is/o/yFhaw/https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) requiring the “watermarking,” or clear labeling, of AI-created content, which is also required by the EU’s recently-passed [AI regulation](https://archive.is/o/yFhaw/https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai).

Interpretable AI is very important
>The lesson here is that while there will be progress on transparency, the black box problem of AI will remain. Each application area will need to develop initiatives geared towards building transparency, which will help ease the adoption process. For example, to help build confidence among radiologists noted earlier, [“interpretability” of AI](https://archive.is/o/yFhaw/https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10546466/%23:~:text=Interpretability%20of%20AI%20systems%20have,in%20different%20fields%20of%20medicine.) — that is, being able understand the cause of a decision made by an algorithm — with radiological applications is a crucial and growing research field to support clinical practice and adoption.

Hallucination
> AI hallucinations have caused models to do bizarre things — from [professing being in love](https://archive.is/o/yFhaw/https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html) with their users to claiming to have [spied on company employees](https://archive.is/o/yFhaw/https://futurism.com/the-byte/bing-ai-spied-microsoft-employees-webcams). Many AI producers have developed a range of mitigation techniques. For example, [IBM recommends](https://archive.is/o/yFhaw/https://www.ibm.com/topics/ai-hallucinations) using high-quality training data; setting clear boundaries on the use of the AI model; using data templates to facilitate output consistency; and continuous testing and refining. Regardless of the actions taken, research suggests that there is a [statistical lower-bound](https://archive.is/o/yFhaw/https://arxiv.org/abs/2311.14648) on hallucination rates, which means that there will always be a chance of hallucinations appearing. Once again, as is logical for probabilistic models, regardless of the quality of the model architecture or dataset, hallucination incidents can be expected to go down but can never be eliminated.

Unknown unknowns is similar to rugged frontier
>AI hallucinations have caused models to do bizarre things — from [professing being in love](https://archive.is/o/yFhaw/https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html) with their users to claiming to have [spied on company employees](https://archive.is/o/yFhaw/https://futurism.com/the-byte/bing-ai-spied-microsoft-employees-webcams). Many AI producers have developed a range of mitigation techniques. For example, [IBM recommends](https://archive.is/o/yFhaw/https://www.ibm.com/topics/ai-hallucinations) using high-quality training data; setting clear boundaries on the use of the AI model; using data templates to facilitate output consistency; and continuous testing and refining. Regardless of the actions taken, research suggests that there is a [statistical lower-bound](https://archive.is/o/yFhaw/https://arxiv.org/abs/2311.14648) on hallucination rates, which means that there will always be a chance of hallucinations appearing. Once again, as is logical for probabilistic models, regardless of the quality of the model architecture or dataset, hallucination incidents can be expected to go down but can never be eliminated.

[[asset/2ea18948e60ecc411775686cc9d0bf58_MD5.jpg|Open: IMG_8066.jpg]]
![[asset/2ea18948e60ecc411775686cc9d0bf58_MD5.jpg]]
### AI use backlash
[https://www.theguardian.com/commentisfree/article/2024/jul/27/harm-ai-artificial-intelligence-backlash-human-labour](https://www.theguardian.com/commentisfree/article/2024/jul/27/harm-ai-artificial-intelligence-backlash-human-labour)  
  
"The backlash, though, points out that we cannot ignore real harms  
today in order to take technological gambles on the future. This is  
why companies such as Nintendo have said they will not use generative  
AI. It is why users of Stack Overflow, a Q&A website for software  
engineers, rebelled en masse after the platform struck a deal to allow  
OpenAI to scrub its content to train its models: users deleted their  
posts or edited them to fill them with nonsense. It is why people have  
started attacking driverless taxis on the streets of San Francisco,  
shouting that they’re putting humans out of work."