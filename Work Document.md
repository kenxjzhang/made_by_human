**2025-08-04 15:37**
I change the timeline to March 10 because the latest FY2024 comes out March 5

**2025-08-07 15:50**
Remember to take the position of disclaimer to top and bottom
Total condition = 9 * 2 (top/bottom) * 4 (4 different reports)

**2025-08-08 09:59**
What are the things need to be careful?
- Make sure the manipulation works and don't make errors
- Pre-Register the study
Basically that's all, no?
I should start to write down the actual parts of the paper

**2025-08-08 15:09**
Got the humanized reports back.
Work on the pre-testing for the reports.
How long will they need to finish the reports, and how much should we compensate them?
I remove the AI disclosure part
Would it be too long for people to read two reports? What's the pros and cons for just reading one report?
How much money should we pay them?
Review the title, description, and the introduction of the survey.

**2025-08-10 10:45**
Write down the working part

**2025-07-06 20:16**
Another thing maybe worth testing is the spillover effects of seeing the AI disclaimer on other competing products.
People pay less money if they know it is a AI-assisted product.
It could be that people have different distributions towards the AI product and non-AI product.

Distribution of WTP:
Non-AI product: 20-100
AI product: 0-100 (Because there is some massive automated AI products with zero costs)

People are just pay rationally by their mean expectations about the product distribution.


**2025-08-06 15:10**
Organizations are hesitant to disclose information.
When disclosing information is not mandated, organizations tend to keep the information inside (_request source but let's make the claims for now_)
However, we do see that organizations are willing to disclose information to achieve XX, YY, ZZ (some benefits for voluntarily disclose inside information)

Remember that the grind is when we have most neural plasticity, good for the brain


**2025-07-01 10:43**
Okay, let's try to finalize the work.
- [ ] What are the industry?
	- [ ] Real estate?
- [ ] What are the companies?
- [ ] How to write? 
- [ ] What numbers should be included?

**2025-07-19 10:14**
### Introduction 
4 different company names:
- Veridian Equity Research
- Crestview Capital
- Stone Harbor Advisors
- Fathom Analytics
### Names of analysts
Does it necessary to have analyst names on the report? Yes, to achieve similar effects that we see in daily life.
If we are going to use the analyst names, the candidates are: 
- J.M. Wright (neutral)
- Christopher Davis (male)
- Sarah Brown (female)

### Report quality
[[Example Reports]]


**2025-07-02 09:33**
If I am able to collect all the project level data from Kickstarter, maybe I can understand deeper about the spillover effect of disclaimer.
Producer decide whether they want to put out the AI use disclaimer on the project. If they claim they have no AI use in their project, there is no 'AI disclaimer' section on their project page.
If they did say that there is AI involvement in their project, then they would have to show it.
Both producer and consumer sides would be interesting.
Producer may not want to call himself out when there is no one doing the disclaimer, because it sounds fishy about posting it.
But it could also be that they want to use the AI disclaimer to assure backers that their use of AI is not egregious.
An example of strategically use of AI disclaimer is "I did use AI in the brainstorming part", which is something not verifiable and can easily conceal.
If producer sees a higher proportion of other producers in the same genre and around same time were doing it, they are more likely to do it.
On the other side, there would be also a critical mass to 'normalize' the disclaimer.
Anyway, it maybe interesting to first document the types of disclaimers that producers use.
Then we can go into each genre and try to understand the strategic use of disclaimer to gain more legitimacy in the crowdsource market.
- [ ] Write about how to use Kickstarter disclaimer in here

### **2025-08-10 15:13** Pre-test report 1
[[Pre-test and Pilot study]]
- People are using different heuristics to judge whether it is by human or AI.
	- For example, some thinks highly structured report is a human signal, while others think it is the AI signal.
>The report is written in a highly structured way. It contains well-explained market context and financial comparisons, which are consistent with human-authored analyst reports.
>It felt overly structured and dry. (chose definitely AI)
>it was well organized like an ai would but cant really tell
>It's highly structured and there's a lack of personal voice

- Many people can not tell the different or paid much attention to the source
>really couldn't tell
>
[[asset/b1adb10987314aa105994500fce35800_MD5.jpeg|Open: image-2.png]]
![[asset/b1adb10987314aa105994500fce35800_MD5.jpeg]]

When people choose "very likely AI", they can't see any obvious cues but rather just because the report is very structured, wordy, and lack subjective opinion
>It's highly structured and there's a lack of personal voice
>It seems very fact based and neutral with these companies information which makes me think it is AI written.
>The vocabulary and the very formal writing style with an excessive amount of details that make it almost not friendly.
>sentence structure, many "ings," some awkward phrases, though not 100% sure it's all AI
>The layout is close to perfect. It comes off as robotic. It is structured very well to the point of me questioning a human has the capabilities to lay something out like this

It is AI because it lacks the imperfection of human writing
>The structure is consistent and polished but lacks the imperfection of human writing.

When people choose "very likely human", they also express the report maybe assisted by AI, especially the structuring
>Different sentiment expressed. Along with weighing pros and cons. It could be a mix of both ai and human
>The report feels mostly human-written, although its polished structure suggest it could have been AI-assisted.
>Most of the text felt very natural. Maybe professionally edited but not by AI (or not heavily so).
>The content and consistency of the subject, data give me a sense that it is written by AI. 
>The formatting felt a bit more like a usual AI format but the writing/text/phrasing itself felt more human so I am slightly inclined toward human.

People recognize it as standard financial reports
>Nothing in it made me particularly suspicious about it being AI generated. It read like a standard, dry financial text.
>The report is a classic investment analysis. Its structure is highly formulaic and follows a predictable pattern. The language is professional and direct, but it lacks unique stylistic flourishes. The text is efficient and error-free. It uses standard financial report phrasing. This combination of traits is common in modern financial content. These are also common characteristics of AI-generated work. You notice this consistency.
>It is very formulaic in its structure, its tone is always neutral, which would lead one to think that it is the work of an AI, but the fact that the financial information they present is accurate and well-contextualized could be a sign of human influence.


2025-08-11 14:37
Remember to add requirements to all the questions to avoid missing observations
- [ ] Need to change the reports to fake names (**important**)
	- [ ] Will come back and finish the report names tonight

2025-08-12 21:32
What if people use AI to check the company names? A concern, but let's check with the pre-testing results.

2025-08-14 10:37
Ready to launch the pilot today


2025-08-14 16:25
About half of the people win the bonus
- Celestial Optics (CELO): 11 (Highest returned)
- Vista Therapeutics (VSTA): 6 (Recommended)
- Adaptive Vision (ADPT): 5

- Hamilton Sterling Group (HSG): 9
- Kinetic Workforce (KNTC): 8 (Recommended and highest returned)
- Apollo Medical (APLO): 2

**Some people got influenced by AI labels, and some do not. Interesting variance in here.**
>It doesn't affect me at all
>Not sure, I mostly made up my mind when I noticed that
>To a minimal extent
>I noticed it but I don't think it influenced me. The text didn't sound AI generated.
>Not much of an influence. That is acceptable to use AI in making the report.
>Not really. I saw simiilar disclosures in my own work and I am sure it will be standard in finance, too, soon

>I was worried about hallucinations/accuracy.
>It influenced my trust of the report. No use of AI made me feel like the author spent time and thought creating this analysis.
>I was less trusting of the report.
>It determined that I needed to pay attention to the other factors besides the AI generated recommendations and made me adhere to critical thinking when choosing the company that I think is best to invest with.
>It gave me pause. In the end, I didn't follow the report's recommendation, so maybe that influenced me a bit, but I don't know if what I would have done if I didn't know about the AI use -- maybe I would have still made the same decision anyway.
>Strongly influenced it especially the fact that both the data/facts and the recommendation came from AI
>It made me question it more. It was all AI so I wasn't sure how much oversight the analyst had.

**Study purpose: some people guess correctly, but it is not too concerning. Most people just think of financial report analysis. Some people think that we are testing how good AI investment reports are, but not "AI labelling effects".**
5/20 people guessed it correctly about our study purpose, but only 3 of them guessed the hypothesis
7/20 people mentioned "AI"



- [ ] Change the question names of investment decisions to make it easier to analyze
- [ ] **Cut the base amount in the introduction**
- [ ] Change the font of the questions

2025-08-14 21:45 with Andras meeting
- [ ] Create the final main study version 
	- [ ] Date should be delayed more
- [ ] Pre-registry
- [ ] Write down the method section about pre-test and pilot study

Write down:
- Write down the pre-test of materials (What are the checks and why we want to do them?)
- Write down the pilot study work flow
Bulletpoints
Readability issues
Find a journal ready paper to mimic the method sections

### 2025-08-15 10:40
Change the date to March 30 and August 30

### 2025-08-21 14:11
I removed the 4 observations with duplicated Prolific ID
Simple analysis of the results: very interesting.
[[asset/c39ca412d855b5eac379bb9e830f6be8_MD5.jpeg|Open: image-7.png]]
![[asset/c39ca412d855b5eac379bb9e830f6be8_MD5.jpeg]]Decision: 
- Do I need to recruit the 4 duplicated participants?
	- Maybe yes. Talk with Andras.
- The penalty effects of disclosure

### Follow-up discussion with Andras after main study:
Do a follow-up study about the partial disclosures
- Randomize the partials
Mechanism?
- Honesty?
Why they don't penalize the yes/yes/yes?
"I trust the disclosure itself is honest"
"I am skeptical of the disclosure reflects the actual use of AI"
Existing scales of misinformation
Google scholar existing perceived truthfulness scales about statement
What can be other explanations?
People may assume that analyst uses AI in the first place
Do people believe the disclosure differently?
**Write out the story with four conditions: no/no/no, mixed, yes/yes/yes, None**
Do people believe or don't believe the disclosure itself?
Make it concrete by writing!
I can work on the single measurement instead of scales
Puzzle: Why they don't care about the "how"?
Labeling products in Marketing literature
Expert decision
### 2025-08-22 09:43
### How to explain the finding patterns?
Working on the story about why we see the patterns
Pattern 1: Most AI disclosures get penalty (measured as following advice)
Pattern 2: Only Yes-Yes-Yes is not penalized
Pattern 3: It doesn't matter how AI is used in the report

If we are going to test the "disclaimer penalty", which alludes that the contents do not matter, it is the presence of disclaimer triggers the negative feedbacks.
Then we can just fill in some very very unimportant things in the disclaimer to test it
Previous disclaimer: Gathering information-Analysis-Recommendation
New disclaimer: Grammar - Formatting -  
Do we need a "Placebo disclaimer" to examine the story?

**Different stories:**
Consistency
Honesty/Suspicion
Lack of competence

Let's write about them by their logics

**Consistency:**
The consistency can explain why people like all Yes more, but can't explain why they don't like No-No-No
Nevertheless, we can think about the logic in here
People may feel uneasy when they need to think about the actual information contains in the disclosure
People regards disclosure as a "nonchalant" thing
But when you actually provide information in the disclosure, they feel uneasy

**Honesty:**
The consistency and lack of competence assume that people believe the actual use of AI is aligned with the disclosure
However, honesty concerns that people may not believe the disclosure reflecting the real use of AI in the report
The suspicion can be towards the organization or analyst
People think "they say they only use AI in information gathering, but I don't believe it" or "Why would they only use it in information and recommendation, but leaving the analysis out of AI? It does not make sense"

**Lack of competence:**
It starts with the presumption that everyone in the industry uses the AI 
Why you don't use it everywhere?
It is actually testable in the real world data -- the perception of focal firm using AI (or disclose using AI) depends on the surrounding firms behavior 

What would the "placebo disclaimer" do in testing the theory?

[[@yangMyAdvisorHer2025]]
In Yang et al. (2025, MS), they find that bank customers are more likely to align their final investment decision with advice from human-AI collaboration, compared to pure AI conditions.
What causes the difference? How to explain the difference?
Their paper and my paper are both in 'financial advice' context.
So it is not a difference in 'empathetic' emotional feelings
It could be the 'disclosure'? How do we disclose things are very important. Let's see how they label their information.
The Central route (rational and compelte) and peripheral route (subtle and social cues)

==I think the paper is focus on making the instruction explicit, rather than deception. They put the instruction upfront, rather than putting it at the end of the report. It may cause different mechanisms==
I think what Andras' intuition is right: the difference between my study and other study is the 'belief' part. I did not include the source of information as part of the study instruction (which would have no reason to be skeptical about for participants). Instead, I put it at the end of the materials, which makes it some how out of the 'study instruction' and something being skeptical about.

Three conditions: put it in instruction - put it as disclaimer upfront - put it as disclaimer below

If we can find some difference between putting in the instruction and make them believe it (e.g. "We collaborated with an investment firm and made sure that the report is generated in such a way.") and putting in disclaimer, it could be a contribution to the literature of how the previous experiments overlooked the "agency" of the participants.

“Revelation” and "Self-Revelation" are different things
In most experiments, we conducted 'revelation' instead of 'self-revelation'.

How are they different?
We may need to go beyond the label?

**These scenarios may be completely different:**
I read a novel.
I asked ChatGPT to generate a novel and I read the novel.
I read the novel and other people told me it is AI-generated.
I read the novel and the author says that some sections were done by AI.

The last scenario seems to be interesting.
Would people worried about 'contamination'?
Why would people prefer to follow the advice more in the all AI situation?

Voluntary and involuntary disclosure by the focal organization is interesting

### 2025-08-26 09:00
It is a good manipulation to me.
Evidence: people pick up the cues of AI involvement in the report. 
1) The estimated efforts and preparation time of No disclosure is close to no-no-no.
	This is actually quite interesting: the default efforts are the same between no-no-no and no disclaimer.
	What's in people's mind when they see no disclaimer? Do they think there is no AI used?
	More likely, it is because different routes of thinking.
2) The yes-yes-yes group has lowest preparation time and effort.
[[asset/5a548cd5d8587c82e67bce31d8b7c2c9_MD5.jpeg|Open: image-7.png]]
![[asset/5a548cd5d8587c82e67bce31d8b7c2c9_MD5.jpeg]][[asset/dbb3d6bc4356d7d807e1d91b769fdd37_MD5.jpeg|Open: image-7.png]]
![[asset/dbb3d6bc4356d7d807e1d91b769fdd37_MD5.jpeg]]

My current research design did not consider the measurement of 'author characteristics'
Maybe a good thing to go?


2025-08-27 10:00
Working on the things Andras mentioned in [[Emails from Andras#2025-08-27]]

"Source Credibility" and "Message Credibility": good distinction. We have measured the trust towards the report, but what about the people behind them? I think I have seen a paper or two measured that. I can find them and see what are the differences.
Lots of econ papers are talking about signaling—we can infer the characteristics of author by the product they produce. In management, lots of the works are the opposite—we infer the product quality by some producer characteristics (gender, demographics, education).


"Trustworthiness" and "Expertise"
>Despite the proliferation of different scales and labels, a remarkably consistent finding emerges from the vast body of factor-analytic research: source credibility is fundamentally a two-dimensional construct at its core. Across dozens of studies, in varied contexts, two primary factors consistently and robustly appear: **Expertise** and **Trustworthiness**

- [ ] Measurement for how participant guess their actual use of AI
	-  A slide for how much they use?
	- "We send the report to the latest AI detection software -- How much do you guess the AI percentage?" (*We may do incentivization in here as classic behavioral econ*)
	- Another benefit of doing it, is that we can also see how people perceive differently when there's another section using AI but not the section we measure
	- For example, 'No-No-No' versus 'No-Yes-Yes'; when we ask what are the percentage do you think from information section (both labeled No AI use in the disclaimer), they may give different answers. This is what we want to measure about the 'contamination effects'
- [ ] Measurement of 'source credibility' towards analyst/firm

[[asset/ed5fb8f3a286984bb556c34ac3273578_MD5.jpeg|Open: image-7.png]]
![[asset/ed5fb8f3a286984bb556c34ac3273578_MD5.jpeg]]

2025-08-28 09:21
Directly measure the 'source credibility' and 'report credibility'


2025-08-29 10:11
Yang et al. (2025) seems to be very related: advice following, 'trust towards report'.
But they did not measure 1) the credibility of the AI statement because it is given by researcher, and as we stated, there could be interesting interactions in there; 2) cognitive trust (trust on quality) and emotional trust (non-quality related trust)
[[asset/28f76f890353ddd8438a13ab467b778b_MD5.jpeg|Open: image-7.png]]
![[asset/28f76f890353ddd8438a13ab467b778b_MD5.jpeg]]


It is possible to see the alignment to advice goes down with human's contribution
>The  impact of human involvement through this route can vary; it may either enhance or diminish consumers’  adherence to the advice, depending on their perceptions of the human’s contribution to the overall quality of  the advice (Petty and Cacioppo, 1986). The literature offers evidence supporting both potential outcomes.

The ELM theory: central route and peripheral route
Song et al. (2023):
>As a classic theory to describe the [information processing](https://www-sciencedirect-com.myaccess.library.utoronto.ca/topics/social-sciences/information-processing "Learn more about information processing from ScienceDirect's AI-generated Topic Pages") process, the elaboration likelihood model (ELM) is often used to explain the influence of product types and online reviews on [consumer attitudes](https://www-sciencedirect-com.myaccess.library.utoronto.ca/topics/social-sciences/consumer-attitude "Learn more about consumer attitudes from ScienceDirect's AI-generated Topic Pages") and behaviors. ELM refers to the path consumers follow to process information ([Sher & Lee, 2009](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0148296323003296#bib632)). This [social psychological theory](https://www-sciencedirect-com.myaccess.library.utoronto.ca/topics/psychology/social-psychological-theory "Learn more about social psychological theory from ScienceDirect's AI-generated Topic Pages") suggests that consumers’ information processing and [attitude change](https://www-sciencedirect-com.myaccess.library.utoronto.ca/topics/social-sciences/attitude-change "Learn more about attitude change from ScienceDirect's AI-generated Topic Pages") can have a dual process. In the “central route,” consumers devote energy to thinking about product-related information and conduct in-depth thinking and deduction of information according to causal logic. In contrast, the “peripheral route” is where consumers only make simple inferences to consider issues, relying more on heuristics, representational cues, and environmental factors in attitude formation ([Filieri & McLeay, 2014](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0148296323003296#b0160)). [Chen, Kim, and Lin (2015)](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0148296323003296#b0080) pointed out that under the influence of the online environment, consumers expect to obtain emotional satisfaction in collecting information, so they unconsciously use less cognitive effort when processing information about practical products to make decisions. [Hlee, Lee, Yang, and Koo (2019)](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0148296323003296#b0235) used ELM to interpret the moderating role of hotel type in the influence of online review style concerning consumer decision-making. [Schulze, Scholer, and Skiera (2014)](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0148296323003296#b0500) also used ELM to explain that when consumers buy utilitarian products, they are no longer as susceptible to online consumer recommendations as hedonic products. This change occurs because consumers choose the “peripheral path” to process information, thinking that recommendations from strangers online cannot truly reflect the performance of utilitarian products. What cannot be ignored is many companies use consumer evaluation information and manipulate word-of-mouth ([Dellarocas, 2006](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0148296323003296#b0125)) for viral marketing, which is also a kind of online reviews. Therefore, this paper uses ELM to reveal the difference in the influence of fake reviews on consumers’ online purchase intention for different product types.


2025-09-02 10:36
Here are the things I found helpful from Long el al. (2025) [[@longDisclosureDilemmaHow2025]]

==Theory of why transparency may not always lead to good feedback from Long et al. (2025)==
>Although transparency is a generally  recommended approach for anything that may concern one’s audience, research shows that  disclosure of AI use may significantly undermine the message and its sponsor.

==Theory of why No-AI use disclosure may invite people to scrutinize the content in Long et al. (2025)==
>From a strategic communications perspective, organizations may consider explicitly  denying AI involvement as a tactic to enhance trust and reassure suspicious audiences. Through  the lens of warranting theory, such denials could be viewed as an attempt to establish authenticity and human authorship as "warrants" of message credibility (Hancock et al., 2020).  However, such denials may also activate increased scrutiny. When an organization specifically  states that no AI was involved in creating a message, it potentially invites audiences to actively  evaluate the message for signs of AI involvement (Buchanan & Hickman, 2024).  From an information processing and persuasion standpoint, labeling content as solely  human-authored may serve as a credibility heuristic (Dehnert & Mongeau, 2022). In real-world  strategic communication contexts in which audiences are likely to be exerting relatively low  effort when engaging with owned or paid media, these signals could be valuable if noticed. On  the other hand, especially if audiences are not assuming their media environment may include  undisclosed AI use, such denials may backfire by distracting attention from the substantive  message or even raising suspicions.

==The results show no difference between denying AI involvement and not mention it. But in my study there is a significant difference. Why?==
>Our test of RQ5 found no significant difference between explicitly denying AI  involvement and simply not mentioning it, though both approaches yielded higher message credibility than disclosure. This provides partial support for the replicant effect (Jakesch et al.,  2019), as content explicitly labeled as human-created was evaluated more positively, but the act  of denying AI use did not provide additional benefits beyond passive non-disclosure. This  suggests that in the current media environment, audiences may not be actively suspicious about AI involvement in health messages unless prompted. The lack of added benefit from explicit  denial is noteworthy from a practical perspective, as it implies organizations need not go out of  their way to emphasize human authorship—simply avoiding mention of AI may be sufficient to  prevent immediate credibility penalties. Given the estimate associated with active denial was  higher, though, we do not want to make a strong conclusion that the two strategies are  equivalent.


### 2025-09-02 17:46
Counterfactual effects of "what if AI is used in the 'no' part?"
Re-read the report?
Willingness to pay?
"As long as one No, there is a conjecture that it could be better"
General AI attitude?
Go back and see the heterogeneity in regression

Age?

Value 'human+AI', "AI"

### 2025-09-04 14:25
Framing needs to change?
- [x] Timetable and presentation
	Oct 24
- [ ] The measurement is hard to adapt from literature
Multi-layered evaluation system

**Google doc for AI Mechanism:**
https://docs.google.com/document/d/1e3vrFg7Um_sWMIzQqO9yyqEqgBG9pAT6Qw-tXhPpQHE/edit?tab=t.0

### 2025-09-05 09:24
- Today continue to search for specific measures
- Also think about what are the other mechanisms to explain the pattern

Metzger et al. (2010) shows that the credibility is mostly through heuristic/non-deliberate way
>Results also indicate that rather than systematically processing information, participants
routinely invoked cognitive heuristics to evaluate the credibility of information and sources
online.



### 2025-09-07 10:16
P229 in Altman(2023) Lay people's evaluation of expert in finance uses peripheral cues; ==but how they picked up the cues are not explored too much==


### 2025-09-09 10:48
I look into the literature review on AI advisor in advice taking
Pretty good summary, it is directly relevant to the 1) AI, and 2) advice taking
I don't find lots of 'label' or 'disclosure' are about the human-AI interaction
Most of the transparency is about "how AI works" but not about "how AI and human work together".
Nevertheless, I should look into the "algorithmic transparency" a little bit because they have interesting results of how increasing the transparency of algorithm would decrease the advice taking

### 2025-09-10 09:22
New articles from IS/OB about advice and AI:
**A very good literature review on advice and AI:**
Baines et al. (2024)

Lehmann et al. (2022)
Bertrand et al. (2022)
Glikson and Woolley (2020)
Rossi and Utkus (2020)

**Transparency of algorithm cause problem in advice following:**
Saragih and Morrison (2022)
Willford (2021)
You et al. (2022)

2025-09-11 08:48
[[@longDisclosureDilemmaHow2025]] is maybe the closest paper with us
[[@toffTheyCouldJust2024]] is also good

2025-09-12 14:22
Attitudes towards AI efficiency
General idea of AI
"Fully use" or "Some areas"

	"Prior expectation of AI use in finance industry"
Where does the incompetence coming from?
"Replacement or Augmented"?


### 2025-09-17 10:50 Changes in Qualtrics for Experiment #2:
- [x] Adjust two bonus instead of 3
- [x]  Introduction change date to Sep 30 and two bonus
- [x] Change date to Sep 30 and lottery in each report and main investment question block
- [x] I also change the date in the report (HR_AI) to match the 'recommendation in Sep'
- [x] Add questions for Mechanism 1,2,3
	- [x] Randomization
- [x] Change the AI questions in Demographic section
- [x] Change the survey flow
- [x] Randomize the order of analyst and firm questions; Also within the question, I randomize the options.

### 2025-09-18 11:34 Chatting with Kevin
AI is obviously better; then show me people don't like it
The prior knowledge is quite interesting; I will measure it in the second experiment
Incorrect beliefs
Disclosure about GMO product


### 2025-09-23 13:38 Revise the final design and do the qualtrics
- [x] Revise the Google Doc and have a clean version of questions
- [x] Implement the Qualtrics
	- [x] Conduct and randomize the Mechanism 1 questions

Stefan:
Neutral about transparency
What is the mechanism? I mean, what is the deeply about how seeing an AI disclosure change our evaluation to a product? What's the connection and distinction from seeing a gender or race information? How does it differently from seeing a qualification for environmental or certification of some kind?
Stefan also mentioned an interesting potential field experiment using board game to facilitate the gender homophily

