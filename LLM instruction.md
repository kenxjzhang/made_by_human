### **Comprehensive Project Overview: User Trust in AI-Generated Financial Reports**

**Project Title (Working):** The Disclosure Penalty: Trust, Transparency, and Coherence in Human-AI Collaboration for Financial Advice.

**1. Core Research Question & Motivation** This research investigates how disclosing the use of Generative AI at different stages of producing a professional investment report affects an investor's trust and subsequent decisions. Motivated by conflicting findings in the literature on "algorithm aversion" and "algorithm appreciation," this study moves beyond a simple "AI vs. Human" comparison to understand a more realistic scenario: human-AI collaboration. The central question is to identify _why_ and _under what conditions_ users trust or distrust content that is explicitly labeled as having AI involvement.

**2. Study 1: Main Experiment & Key Findings**

- **Objective:** To measure the causal effect of stage-specific AI disclosure on an investor's reliance on a financial report's recommendation.
    
- **Methodology & Implementation:**
    
    - **Design:** A **2 (Information Gathering: AI vs. Human) x 2 (Analysis: AI vs. Human) x 2 (Recommendation: AI vs. Human) between-subjects factorial design**, plus a **ninth control condition** that received **no disclosure statement**.
        
    - **Participants:** ~1,400 adults with self-reported investment experience were recruited from the Prolific platform.
        
    - **Procedure:** Participants were randomly assigned to one of the nine conditions. They read a professional-style investment report analyzing three anonymized companies and concluding with an explicit recommendation. The report's content was identical across all conditions; only the "Generative AI Disclosure" at the top of the report varied. After reading, participants made an **incentivized investment decision** (with a real monetary bonus for a correct choice), followed by survey questions measuring potential psychological mechanisms and demographics.
        
- **Observed Patterns & Key Findings from Study 1:** The results revealed two surprising and significant patterns:
    
    1. **A "Disclosure Penalty":** The mere presence of a disclosure statement—even one stating the report was entirely human-made (`no-no-no`)—significantly reduced the rate at which participants followed the investment recommendation compared to the control group that saw no disclosure (`none`). The trust penalty was not specific to AI, but to the act of disclosure itself.
        
    2. **The "Messy Middle" is Penalized, Not "Pure AI":** Contradicting a simple "AI Aversion" hypothesis, the condition where all three stages were disclosed as AI-driven (`yes-yes-yes`) was the _only_ disclosure group **not** to be significantly penalized. Its performance was nearly identical to the "no disclosure" baseline. The lowest rates of advice-following were observed in the mixed human-AI collaboration conditions.
        
- **Interpretation of Study 1:** The findings suggest that users do not penalize AI use itself. Instead, they appear to penalize processes that are **ambiguous, inconsistent, or have unclear ownership**. A fully automated process (`yes-yes-yes`) is perceived as a coherent, transparent system. The mixed conditions represent a "messy middle" where accountability is diffused. The initial set of measured mechanisms did not fully explain this complex pattern, motivating a follow-up study.
    

**3. Study 2: Follow-up Experiment (Design Phase)**

- **Objective:** To directly test three new, more nuanced psychological mechanisms that could explain the surprising results of Study 1. 
    
- **Methodology & Design:**
    
    - A simplified **4-group between-subjects design** will be used to efficiently test the core comparisons:
        
        1. **Group 1: No Disclosure** (The high-trust baseline)
            
        2. **Group 2: `Human/Human/Human`** (The "Explicit Human" condition)
            
        3. **Group 3: `AI/AI/AI`** (The "High Coherence" AI condition)
            
        4. **Group 4: `Human/AI/Human`** (A representative "Messy Middle" / low-coherence condition)
            
- **New Mechanisms to be Measured:** This study will measure the investment decision as before, but will deploy new scales to measure the following three mechanisms:
    
    1. **Perceived Process Coherence:** The degree to which the described creation process feels seamless, integrated, and professional versus disjointed and piecemeal. The hypothesis is that the `AI-AI-AI` condition is perceived as a high-coherence process, while the `Human-AI-Human` condition is low-coherence.
        
    2. **Perceived Objectivity:** The belief that the report is based on impartial data and logic, free from subjective human biases. The hypothesis is that the `AI-AI-AI` condition is perceived as more objective than the mixed conditions where a human might use AI to selectively support their biases.
        
    3. **Disclosure Credibility (Suspicion of Truth-Telling):** The extent to which participants believe the disclosure statement itself is honest and complete. The hypothesis is that the `AI-AI-AI` disclosure is seen as a costly and therefore honest signal, while a `Human/Human/Human` claim in the modern era might trigger suspicion that the firm is concealing its true AI usage.