# Emails from Andras

### 2025-8-28
Yes, that makes sense. I guess part of this is due to the experimental  
setup. If you used a very well-known firm (e.g., Goldman Sachs),  
perhaps that would matter in how people interpret the AI disclosure.  
Here, the firm/analysts are not well-known, so that's where the  
uncertainty lies.  
  
That's a good start on measures. Keep a running list of candidates  
(and where they came from), and we can then discuss and pick the right  
set. It's nice if there is an existing scale that we can adapt  
(without having to mix items from different scales), so keep that in  
mind. But sometimes it's hard to do that, especially when measuring  
something new, so sometimes you need to adapt items from several  
sources. (AI deep research, for some reason, likes to mix items from  
various scales).  
  
I look forward to seeing how you might measure the other mechanism.  
(Perhaps some of those measures should focus on the analyst and others  
on the firm?)  
  
And as I mentioned, it might be helpful to have some measure of report  
credibility, too, because your story might be something like this:  
  
Pathway 1:  
Perceived credibility of AI disclosure --> Perceived credibility of  
report --> Decision whether to follow report's recommendation  
  
Pathway 2:  
Perceived credibility (or sophistication, etc.) of analyst/firm -->  
Perceived credibility of report --> Decision whether to follow  
report's recommendation
### 2025-08-27
Hi Ken,  
  
That's a really nice additional distinction from prior work: 1) You have information-analysis-recommendation task variation within a single material, and 2) you have 'self-revelation' instead of 'revelation'. What's nice is that both of these are (a) more realistic (e.g., it's usually self-revealing, as you noted, in real life, and also, with the rise of AI-human collaborations, the most relevant question is usually not whether AI was used but how/when/where AI was used).  
  
Beyond the explanation centered on the “perceived credibility of AI use,” another plausible mechanism that you mentioned earlier may deserve some attention. As you suggested in our first conversation after reviewing the results, the very presence of a standardized, firm-mandated disclosure about AI use (compared to the absence of a disclosure) may serve as a salient reminder that sophisticated analysts and leading firms in finance increasingly rely on AI across the entire analytic process. If so, a disclosure that specifies no or only partial use of AI might not simply reassure readers about honesty or transparency; it could instead signal technological backwardness. That is, even if participants believe the disclosure, they may infer that an analyst failing to deploy AI fully across data collection, analysis, and recommendation formulation is less sophisticated, slower to adapt, potentially a laggard, etc. In this way, the disclosure may paradoxically undermine credibility by highlighting the analyst’s relative lack of technological advancement. That may explain why Yes/Yes/Yes does well (it signals technological sophistication) and why no disclosure also does well (as it doesn't make AI and technology-related expectations salient and, in fact, allows readers of the report to assume whatever they would like to assume about the analyst's technology use and technology advancement).  
  
These are two distinct mechanisms because the first one is about people *not believing* the disclosure (and thus being more skeptical of the whole report) unless it's Yes/Yes/Yes, while the second one is about people *believing* the report but regarding any disclosure other than Yes/Yes/Yes as a generally negative signal about the sophistication/capabilities of the analyst (or, relatedly, the report at hand or even the whole firm).  
  
Sometimes, both potential mechanisms happen at the same time (e.g., some people don't believe the no/partial AI use labels, and others believe them but don't have a great opinion of firms/analysts with no/partial AI use).  
  
In a smaller follow-up study, you could measure these two mechanisms, and without changing the fundamental design and setup you used in the main study. (I agree that signalling that the firm's disclosure was "verified" could open a Pandora's box of other changes). So, the most straightforward approach might be to keep what you did (except that you'd just have four basic conditions: Yes/Yes/Yes, No/No/No, Partial AI use, no disclosure) in your setup, but measure these two mechanisms directly, perhaps similar to the way you measured the mechanisms in the main study (you could also randomize whether the mechanism questions come before or after the main, incentivized DV, to randomize away any possible order effects).  
  
I am sure there are extant measures of perceived disclosure (or, more broadly, message) credibility/transparency/truthfulness you could adapt to measure if people (dis)believe the disclosure. For example, in marketing and advertising research, but also, I know there is research on media/the press focused on message credibility and source (or medium) credibility. You'd probably also want to measure the credibility/trustworthiness of the report/analyst/firm itself, so you could see if the lack of credibility of the AI message is associated with skepticism about the report/analyst/firm overall (which in turn might be associated with the decision not to follow the report's recommendation). So, I think it might be good to measure (a) disclosure credibility and (b) (what they sometimes call) "source credibility" (where "source" is the report, the analyst, or the firm), as it might be that (a) --> (b) --> your main DV.  
  
For the other measure, you could look at existing measures of perceived innovativeness, perceived technological sophistication, perceived organizational capability/competence, perceived adaptiveness, and things like that. And might also look at the existing measures of perceived ability of the analyst, etc.  (And again, it might be that these measures --> source credibility --> your main DV).  
  
You could also consider single-item, sliding-scale measures (0 to 100). These can be acceptable if you have unidimensional, intuitive constructs (e.g., perceived credibility of a disclosure might fit the bill), a single carefully designed item can perform nearly as well as a multi-item scale. And when respondent fatigue is a concern, as here, it might be helpful. (For example, see [here](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2018.3243?casa_token=Sxi0YesFdLUAAAAA:Wy80chtjv51KGATnxZhvDBAnRROMc3hhXcu67qcQ0oD-vA4tXzBxOE28hN6_UbmV3dX8hCQGCg) for their sliding scale measure of fit). Sometimes you get a lot of noise on mechanism measures because people get tired of too many, seemingly similar items and stop paying attention as much; see [here](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0207484). So I could imagine an approach where people see just, say, 5-6 sliding scales that are generally similar to each other (e.g., "How truthful was the AI disclosure?" "How innovative is this firm?" "How trustworthy is this report overall?" etc.)

  
I hope this makes sense. Happy to chat more, soon.  
  
András  
  
P.S. It's interesting that we're coming back to the question of whether people believe the AI disclosure given to them. It's something you raised a long time ago as a potential empirical issue, but it looks like it's emerging as a potentially major theoretical point here. :)

### 2025-8-21
Thanks! 

That's odd about the duplicates; that doesn't usually happen! Perhaps it has something to do with the glitches this week. (They finally just fixed the problem yesterday, so I now have access to my standard workplace within Prolific again). Should I ask for four more respondents? It's easy to do so, and that would give you the full, preregistered N = 1,350. Let me know!

  
Happy to have a chat soon. I could do tonight at ~9pm on Zoom, and I also have some time on Monday and Thursday next week to meet in person (I'll be at Rotman for some of both of those days). 

  

Very interesting (and unexpected, at least to me) patterns. And potentially policy relevant, too, with all the talk about AI disclosures in firms and policy circles.

  

In part, this reminds me of the idea of a disclosure penalty (see [here](https://faculty.tuck.dartmouth.edu/images/uploads/faculty/daniel-feiler/Sah_Feiler_-_Disclosure_penalty_and_the_altruistic_signal_-_2019.pdf)), but not entirely, because people don't penalize Yes/Yes/Yes disclosures. Maybe what's driving this is that if a firm says "we don’t use AI" (i.e., No/No/No) or “we only used it in some areas," that could be interpreted as downplaying true AI involvement. People know AI use is widespread, so claiming "none" or "just in some areas" may sound like a firm that is trying to minimize exposure, not reveal the whole story. That could explain why Yes/Yes/Yes isn't penalized. In contexts where AI is now expected, “we don’t use AI” may seem implausible. (People might think: "Really? In 2025, an analyst  does not use AI at all?") Similarly, "we used AI in just one or two stages" might feel technically possible but sort of odd ("Why would a firm not integrate it more systematically?"). And so perhaps there is a kind of honesty penalty, whereby people don't trust the _disclosure_ _itself_ if it insists on "no AI" or "only some AI" and that might make them less likely to follow the report's recommendation. (And, in contrast, there is little reason to doubt the honesty of the disclosure when it's Yes/Yes/Yes (why would anyone lie about that?), and there is also little reason to worry about AI/disclosure issues when there is no mention of AI at all.

  

The current mechanism measures weren't directly designed to capture possibilities, but you could do a follow-up study, with a smaller sample (e.g., the conditions could be 1. No/No/No, 2. Yes/Yes/Yes, 3. partial disclosure (randomized across the various partial disclosures, but pooled together to keep the sample smaller), 4. [no disclosure])) and some mechanism measures. Or some other approach.

  

András

### 2025-08-18 09:13
1. This one is forthcoming in Management Science (by operations researchers):  [https://arxiv.org/pdf/2506.03707](https://arxiv.org/pdf/2506.03707)

It compares how people compare pure AI advice vs. human-AI advice (where human has a final say over the advice) and finds (unsurprisingly) that people prefer the latter. That is, they find that people want a human in the loop versus just pure AI-generated advice. 

  

That's not surprising (and, from a quick look, this paper doesn’t seem to look at how these compare to pure human advice). Most importantly for your purposes, it doesn’t look at variation in _how_ AI was used; i.e., it takes "human-AI advice" as a homogenous thing rather than looking at the use of AI for different tasks, which I believe is your distinct contribution.  
  
So, even though it sheds light on the importance of understanding "human-AI collaboration/augmentation," it doesn't tell us what kinds of human-AI collaboration people are okay with and what kinds they reject.  
  
2.  You might have already seen this: 

[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5186803](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5186803)

  

From the abstract, it seems it finds that people appreciate advice generated by AI, but ex ante, they don't expect that they would appreciate AI advice (i.e., when given the choice, they prefer a human advisor over AI). Like the first paper, it doesn't examine the use of AI for different types of tasks.

  

So, both are helpful in building up your story, but neither lessens your core contribution.  You can work these into your paper while waiting for the results.


2025-7-14
Sounds good. See you then!
As we discussed earlier, another thing you can work on now is a Google Doc draft of your pre-registration ([https://aspredicted.org/](https://aspredicted.org/); see here for helpful guidance and examples on answering the pre-registration questions: [https://datacolada.org/64](https://datacolada.org/64)). I am attaching a few of my own pre-registrations in case they are helpful.

Best,

András

June 14, 2025

Hi Ken,

I don't think fatigue will necessarily be a huge issue, as long as we

compensate participants. But it's hard to judge these things in

advance. We'll run a pretest before the main study, and that usually

is a good source of data on that (e.g., if a lot of people stop

completing the survey or complain in Proflific messages that the

compensation is too low, etc., then we could cut back).

I think what you have here is enough to go to REB, after making some

changes in response to my latest set of comments. If we come across

other versions of these measure, or if we want to adjust them when we

look at them with fresh eyes before launching, we can do so easily.

That would probably fall into a category of minor enough word changes

not to need an REB adjustment.

So, I'd do the following:

1. Go ahead and make some adjustments based on my latest comments in

the Google Doc.

2. Then, check the whole the document and submit it to REB. (Before

submitting to REB, make sure you create a preview PDF document to see

the formatting looks okay, as REB reviewers might look at the PDF). It

will probably take a couple of weeks to hear back from REB (and they

sometimes want a few changes); that's a normal part of the process.

3. In the meantime, once submitted to REB, work on the Qualtrics. (You

should have access to a subscription through UofT, I think, but ask

Manuela et al. if you run into issues).

4. Once done with the Qualtrics and if there is still time, you can

work on the front end of the paper and the Methods section.

I return on July 12. We can run a pretest soon after and, then after

any adjustments we need to makle, launch! Good job getting this to

this stage. I look forward to the next steps. :)

As for AOM, I can cover an additional $500 as your advisor (not as PhD

coordinator). Would that work? (Speaking of money, I'll also

e-transfer you $200 for Pelu-sitting today or tomorrow).

Best,

András

June 15, 2025

Very useful discussion and tips for AsPredicted, with examples:

[https://datacolada.org/64](https://datacolada.org/64)

[https://aspredicted.org/](https://aspredicted.org/)

[https://datacolada.org/44](https://datacolada.org/44)

[https://aspredicted.org/help](https://aspredicted.org/help)

June 12, 2025

Hi Ken,

This is good progress. In the interest of time, I wanted to get back to you as soon as possible; I focused my comments on the non-incentivized (mechanism) questions. Please see the Google doc. But here are a few general notes so you know what my main thoughts were.

1. I like the three possible mechanisms you outlined. I think there are at least two more I would want you to measure. One is the _perceived opacity of AI_ (i.e., the perception that it's more of a black box than human reasoning), which I believe is different from from the perceived lack of accountability (which is an important potential mechanism on its own and has more to do with whether someone can be held responsible for the report). The other is _hallucinations_ (or more generally, perceived reliability/credibility). I talk about both of these in more detail.
    
2. For each mechanism, you might want to use more than one item (and then average them to form an index). Sometimes, single-item measures still have a bad reputation. As you think about measures, I gave you some suggestions of the kinds of things I have in mind, but ideally, you could draw on and borrow/adapt some existing measures. I am sure there are existing items/scales to capture things like perceived effort, perceived credibility, perceived transparency/opacity, etc. Sometimes the extant literature won't have exactly what you need, but often you find something very close to it that you can adapt (and perhaps complement or mix with some of your own items or tweak to fit your context).  3) These two HBR articles ([](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fhbr.org%2F2025%2F01%2Fwhy-people-resist-embracing-ai&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5215a1fb4880462b05f308dda702141f%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638850350188554468%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=eSWbbpJ%2B1m0bSh8%2BxAiOZHVFt4LdG2rFdeiTevHPQT8%3D&reserved=0)[https://hbr.org/2025/01/why-people-resist-embracing-ai](https://hbr.org/2025/01/why-people-resist-embracing-ai) + [](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fhbr.org%2F2024%2F05%2Fais-trust-problem&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5215a1fb4880462b05f308dda702141f%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638850350188579319%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=JQajGYDBYOOhiLizWron4H%2FQCGJtZIDK9xuQ%2BB7GbJ0%3D&reserved=0)[https://hbr.org/2024/05/ais-trust-problem](https://hbr.org/2024/05/ais-trust-problem)) summarize a lot of different concerns people have about AI. Many of these are not all that relevant to your context, but take a look; some of them might be helpful. For example, the opacity mechanism I mentioned above is featured quite prominently in the Why People Resist Embracing AI article.
    
3. As you will see, I didn't think your current Questions 1 and 2 were all that helpful. These just seemed to be non-incentive-compatible (and therefore less helpful) ways of capturing what you'll already capture with an incentive-compatible decision item. (For example, see my experiment in this paper -- [](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdoi.org%2F10.1177%2F0003122420969399&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5215a1fb4880462b05f308dda702141f%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638850350188592987%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=UiSCg3OI3gCws%2BZR2O2MK3KlhbfD2f%2FOhO0IF%2FU6CzE%3D&reserved=0)[https://doi.org/10.1177/0003122420969399](https://doi.org/10.1177/0003122420969399); the main outcome is an incentive-compatible decision -- who would you put on your team, knowing your bonus depends on it --  and there are some non-incentivized mechanism questions, but there was no need to ask basically the same question as the main question again in a non-incentivized way).
    
4. Minor thing, but I noticed in the Why People Resist Embracing AI article a line saying, "For example, people are just as open to financial recommendations from AI as they are from humans, because that task is viewed as objective." For this, they reference this [research study](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fjournals.sagepub.com%2Fdoi%2F10.1177%2F0022243719851788&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5215a1fb4880462b05f308dda702141f%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638850350188605746%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=sZ93lqyQtdKruVRP4PzE%2Fo1OYhIYfMm5Bwyi4LBjxvc%3D&reserved=0). I believe this is helpful for you because it seems it suggest the opposite of the economics working paper we found earlier (where they found people relied more on investment prediction from a human than a human-AI collaboration). Mentioning contrasting findings (both regarding financial recommendations/predictions) can be a great way of building up tension that will motivate your paper. So please make sure you make note of both these studies. You should have a document where you keep track of these studies (with a quick note on what they say and how you can use them) and also save the PDFs. It will be super helpful when writing the paper.
    

For now. I think the next step is to finalize the list of potential mechanisms (e.g., perceived effort, perceived opacity, perceived accountability, perceived credibility, etc.) and then find measures for each of these. Hopefully, you can make progress on that in the next couple of days, and I think you're getting close!

I hope this helps! I am excited about this direction regarding the mechanisms. Happy to chat if any of this is unclear.

András

May 30, 2025

Tips about choosing names:

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fjournals.sagepub.com%2Fdoi%2Fpdf%2F10.1177%2F0146167218769858&data=05|02|kenxj.zhang%40rotman.utoronto.ca|5d35f036798b40c4954208dd9fb4707e|78aac2262f034b4d9037b46d56c55210|0|0|638842319508294530|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|80000|||&sdata=%2F3jnCzdw11yvAQghuox5eG35CQMfpvGgREM0QgD%2BPyo%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fjournals.sagepub.com%2Fdoi%2Fpdf%2F10.1177%2F0146167218769858&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5d35f036798b40c4954208dd9fb4707e%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638842319508294530%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C80000%7C%7C%7C&sdata=%2F3jnCzdw11yvAQghuox5eG35CQMfpvGgREM0QgD%2BPyo%3D&reserved=0)

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.ssa.gov%2Foact%2Fbabynames%2Fdecades%2Fnames1990s.html&data=05|02|kenxj.zhang%40rotman.utoronto.ca|5d35f036798b40c4954208dd9fb4707e|78aac2262f034b4d9037b46d56c55210|0|0|638842319508313486|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|80000|||&sdata=LWS44%2Bkjklu4ilQ%2FftCdG2iGPFJWjObWVag8jXffNp4%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.ssa.gov%2Foact%2Fbabynames%2Fdecades%2Fnames1990s.html&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5d35f036798b40c4954208dd9fb4707e%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638842319508313486%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C80000%7C%7C%7C&sdata=LWS44%2Bkjklu4ilQ%2FftCdG2iGPFJWjObWVag8jXffNp4%3D&reserved=0)

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.census.gov%2Fabout%2Fhistory%2Fcensus-records-family-history%2Ffrequently-occurring-surnames.html&data=05|02|kenxj.zhang%40rotman.utoronto.ca|5d35f036798b40c4954208dd9fb4707e|78aac2262f034b4d9037b46d56c55210|0|0|638842319508326006|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|80000|||&sdata=3%2F%2BNmp0LHZVTtHZoFtoORpuGMxdeUYEBFqzLlhw7FYA%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.census.gov%2Fabout%2Fhistory%2Fcensus-records-family-history%2Ffrequently-occurring-surnames.html&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5d35f036798b40c4954208dd9fb4707e%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638842319508326006%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C80000%7C%7C%7C&sdata=3%2F%2BNmp0LHZVTtHZoFtoORpuGMxdeUYEBFqzLlhw7FYA%3D&reserved=0)

May 30, 2025

[AI mechanisms.docx](attachment:c43c80b0-c5be-4c34-a0bd-623f8bbdb91f:AI_mechanisms.docx)

Hi Ken,

See attached for another approach to the mechanisms. I had this idea

and then asked ChatGPT to make a sketch. It's a kind of post-task

reflection. To make sure it's not cognitively taxing, you could just

use "check all that apply"-style boxes, where people check the boxes

for everything they liked or didn't like (rather than providing

ratings). And it's a format that can be given to everyone, across all

conditions.

These are not all the right items (and there is probably too many),

but something like this might work. And people can just check boxes so

you'll get a % of how many people were concerned about, say,

hallucinations, etc.

András

May 8, 2025

Hi Ken,

That was a productive meeting. So, I guess we agreed on the following:

- Categorical DV (choose one of three firms, but frame it as an _investment decision_ not as a belief/estimation/prediction)
- A better cover story (you are looking to complement your personal investments and have been researching three semiconductor firms; you are looking to invest in one of them as part of your portfolio; you found a graph, and separately also a report from an equity research firm on the semiconductor industry that directly compares the three firms, etc.)
- A filler graph
- Some language to gently remind people that the report is not infallible
- Let's think about the conditions (8 or 15, etc.)

Did I miss anything?

András

May 5, 2025

Hi Ken,

Just a quick note. The idea I was telling you about on Friday (for potentially using a few different version of the report) could be thought of as a form of _stimulus sampling_. I forgot the term, but that's what they call it psychology: [](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdictionary.apa.org%2Fstimulus-sampling&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C2c5483067d6f469ed83c08dd8bdc13d0%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638820499510864184%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=HylN9GHS2eqOK4ju4SvA3EO%2BjT3wLWC%2FNj1C3BjQDZ0%3D&reserved=0)[https://dictionary.apa.org/stimulus-sampling](https://dictionary.apa.org/stimulus-sampling)

In the economics literature on audit studies, people talk about this as a way to avoid _template bias_.  (See: "Another limitation of the audit study is that they are only externally valid for the stimuli that they send. Lahey & Beasley (2009) coined the term “template bias” to describe one aspect of this problem. Template bias occurs when a limited number of stimulus templates are sent out that do not represent sufficient variation. For example, assume a hypothetical labor market in which Hispanic men are only discriminated against if they have mustaches, while non-Hispanic men with mustaches are not discriminated against. If only auditors without mustaches are sent to apply for jobs, this underlying discrimination will not be found. Ideally an audit study will send stimuli that fully explore the feature variation seen in the market and relevant to the study topic.") In your case, there could be a similar kind of template bias if all your reports were AI generated (or human generated) or if they always made the same recommendation, etc.  So introducing some variation (through a form of stimulus sampling) helps avoid that problem.

András

May 8, 2025

Hi Ken,

I added comments to the document. Many of these are small things, but there are a few worth discussing (including the ones you flagged at the end of the document, plus the ones where my comment begins with "Let's discuss" — these overlap to some degree).

As I worked on the document, for some reason, it was corrupted midway through and I haven't be able to open it again even after trying various approaches and computers. But when I upload it to Google Drive, it opened and preserved my comments. So I am sending **[my comments in this Google Doc](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1rxtzU5ojD8qZgsQYj_v_cYWMTt5NZHtn%2Fedit%3Fusp%3Dsharing%26ouid%3D102874455186892240253%26rtpof%3Dtrue%26sd%3Dtrue&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7Cf894981ced624407886b08dd8da1af95%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638822448221565873%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=MTpZB1V4RIefoDi%2FW3YUNCAmA9zL41LAQV9JGO5NWBY%3D&reserved=0).**

In addition, I am attaching REB application materials Sarah and I used for an experiment on Prolific ("EDI Study"). It includes the protocol form (which you don't need to worry about for now) and also the materials we had to attach (consent page, recruitment message,  questionnaire, example of stimulus materials, etc.). Feel free to rely on these as you create your items (e.g., for the consent form or the demographic questions, you can probably borrow a lot).

I am also attaching another REB protocol for an incentivized prediction study ("Prediction Study") that was part of the same project but conducted earlier. You might just search for the word "bonus" in this form to see how we talked about our bonus payment in the REB application.

As the next step, once we figure out the fundamentals, you should work on the materials (consent form, question items, instructions, etc.). You can worry about the REB protocol form itself later. Once the materials are in place, the form itself is quite easy to do.

See you tomorrow,

András

Thanks, Ken.

Yes, I think the reports will need to be written (by you, by AI, by gig workers, etc.) rather than found.

That's fine. There would be some deception involved in there, and that's okay. IRB will be fine with that. And there will have to be a bit of deception anyway if you want to look at the pure label effect.

More important is that, if possible, the options (investments, employees, etc.) should be real (e.g., a bit like in my statistical discrimination paper in ASR: the options participants could choose from were real people with actual GMAT scores—which affected participants' bonus payments). You can only really pay people honestly according to their performance in the task if the options have some real performance attached to them (either past performance that only you know, or future performance that neither you, nor the participants know at the moment but will be clear in the future, e.g., stock market performance in a week).

I like the idea of 3 options. And actually, instead of the third one being a weak filler, it might be best if the three options were similar in quality (both had things going for them). That way, there is less risk that people always choose the option recommended by the report (which might be their prior!).

Sounds good. You just want to make sure you set up the task in a way so that people don't (easily) start looking for information beyond the report.

(It's not terrible if they do, as it might mean they don't fully trust the report—and shows the incentivization to get it right is working—but you don't want external information to be driving the answers!)

So maybe startups or companies from another country would work best. Or you could even give the companies an anonymous identifier or pseudonym (while telling people it's a real company and using a real company); I sort of did that in my stat. discrimination paper in ASR.

May 5, 2025

Hi Ken,

This is a good find. It can be a nice starting point for your contribution, as it can give you a compelling “null” (i.e., a compelling baseline expectation that is currently the conventional wisdom). Recall how important it is to have a compelling null, according to Zuckerman’s tips for article writers.

My main reaction is below, but I just wanted to point out something first. It’s worth noting that some of what they measure is actually quite different from what you’re looking at. In particular, their studies look at the effect of AI disclosure on trust in the _person_ who used AI (e.g., the supervisor or colleague or professor or job candidate) rather than the _output_ itself. Those are quite different. For instance, I might think a paper written with the help of AI is quite good but I might think less favorably of the colleague (as a scholar) who publishes such a paper than a colleague who can publish the same-quality paper without AI help. So one might evaluate the output quite differently than the person. In fact, in the example I gave, the more helpful the AI is for writing the paper (i.e., the better the output), the less impressed I’ll be by the colleague for publishing it.

MAIN REACTION

Okay, so here is my main reaction: This paper is still looking at a very simplistic dichotomy of “prepared by generative AI” vs. prepared by a human. So, I think your big potential contribution is to go beyond that dichotomy. And you’re proposing to do that in two (closely interconnected) ways: (a) recognizing that outputs (and all output-generating processes) have different parts (e.g., information, analysis, recommendation) and judgment of AI use might differ across these; and (b) thinking of ways to explicitly measure AI-human collaboration as a distinct condition (e.g., in the “incentivized decision task” (i.e., incentivized survey), as we discussed, you could easily include three options for AI.

Going beyond the dichotomy in these two ways could be an important contribution. If you look at Ethan Mollick’s book, he makes two points (which others have made, too) that your approach relates to. First, he emphasizes co-intelligence and the idea of being the “human in the loop” (i.e., maintaining active oversight when using AI, e.g., so that human supervision can ensure accuracy or ethical considerations, etc. are upheld). By looking at cases of AI-human collaboration (rather than just pure AI and pure human), you can explore reactions to a condition where output was AI generated with this kind of human supervision. Second, Ethan also talks about the idea that AI technology has a _jagged frontier_ whereby tasks that appear to be of similar difficulty may either be performed better _or_ worse by AI than by humans. By treating outputs as being made up of distinct parts, or being generated in distinct steps, rather than being a single, uniform output, you look at evaluation of AI-aided output along this jagged frontier.

EMPIRICAL IMPLICATIONS

Empirically, this means that I think you should think about making the three conditions—(a) prepared by AI, (b) prepared by human, and (c) prepared by AI with human supervision, or prepared by human and AI in collaboration—as central as you can.

For the incentivized decision task (or survey, but I like “incentivized decision task” more), this is quite simple, as you could ask people to indicate, for each part of the report, which of the three options they prefer.

For the experiment, it requires a bit more thought, but you have options that don’t necessarily require a 3 x 3 x 3 full factorial experiment with 27 cells.

First, you could keep the basic design you are thinking about (i.e., either AI or human author, but not a collaboration both, within each of the three parts of a report), so it’s still 2 x 2 x 2, but you still sort of get at human-AI collaboration (e.g., you might find that people prefer, at the level of the report, AI-human collaboration; for example, they might prefer human data collection and recommendation, but AI analysis).  (If you did it this way and also ran the incentivized decision task, then that should also just have two options for each section—AI or human—so that two studies are consistent).

Second, you could consider a non-fully crossed factorial experiment (in which some combinations of factors are not tested; sometimes people do this if some combinations are uninteresting, impossible, impractical, etc.). This would cut down on the number of cells but still allow you to directly test the most interesting combinations. For example:

Condition 1: Section 1: Human. Section 2: Human. Section 3: Human

Condition 2-8: At least one of the three sections are AI generated (e.g., Section 1: Human. Section 2:  AI. Section 3:  Human; 7 possible combinations in total, right?)

Condition 9-15: At least one of the three sections are AI generated with human supervision, i.e., generated in an AI-human collaboration (e.g., Section 1: Human. Section 2: AI & Human. Section 3: Human; which is another 7 possible combinations, right?)

So, this would give you 15 cells, which is more than 8 (which is what you’d have with the more dichotomous 2 x 2 x 2 setup) but a lot less than 27.

The prediction might be that, consistent with prior research, Condition 2-8 (the “AI conditions”) are less generally trusted than Condition 1 (“the human only condition”). But, potentially, Conditions 9-15 might be more trusted than the Condition 2-8 and, perhaps, interestingly, some of the combinations captured in Conditions 9-15 might be even more trusted than Condition 1.

Relevant to either approach, I think it might be good to keep your contexts of interest (e.g., finance (e.g., investment), HR (e.g., hiring/promotion, strategy (e.g., choice of technology or strategy). But if the paper uses any of those (e.g., it seems they have one on hiring), and the materials seem helpful, you could consider adopting those (though, sometimes, things adopted from psych/OB experiments are not easily amenable to incentive compatibility).

MEASURING YOUR DV

After our chat on Friday, I was also thinking about the issue of how to measure the dependent variable in the context of, for example, a buy/sell or hire/not hire recommendation. I was wondering if one way around the problem would be to give each participant one report but more than one decision option. For example, you give them a report (with an information section, analysis section, and recommendation section) but that one report covers two companies and they need to select which one to invest in. Or the report discusses two people who could be promoted, but they need to select which one. So the recommendation in these cases isn’t “buy” or “promote” but “buy/invest in Company A” or “Promote Candidate B.” This would simplify things because you would _not_ need two reports—just one that covers both options with a recommendation to go with one of the options (Or possibly, there could be more than two options, so that there is more than one non-recommended option people can choose, so they don’t all/mostly choose the recommended one). And then you can simply measure if people followed the recommendations or not.

For example, you give them a report comparing three potential investments (A, B, and C), with a slight recommendation to choose one of them (you would randomize which one). And then your DV equals 1 if they followed the recommendation. And your independent variables are the various combinations of AI/human in creating the output.

In addition to eliminating the need for two different reports, and also ensuring that you have a clear and simple DV, this is very portable across contexts (e.g., you change if the options are investments or strategies or people or technologies) but the DV doesn’t change: it’s the probability that they followed the report’s recommendation.

A FEW OTHER THOUGHTS

The OBHDP paper you sent could be helpful for a couple of other things, too.

First, you should take a look at the alternative explanations they discuss and try to rule out (E.g., Is it AI use or the fact the production is outsourced (to someone/something) that’s driving this? Does the effect depend on the wording of disclosure?). You might not need to worry about all of these (e.g., the outsourcing issue might not apply in your contexts and given your focus on the evaluation of the output itself rather than focusing on trust in the person), but it’s worth a look.

Second, they think about various mechanisms, which you should be very aware of, and see if any of it is relevant/useful.

Sorry for the long email! I hope this helps. 🙂

András

May 2, 2025

Thanks, Ken!

To be honest, I'm not sure about this way (Experiment #3) of combining

them. First, I don't think you'll get much credit for measuring stated

preferences. People say all kinds of things, but if there are no

stakes or a behavioral response of some kind, people might not care

much for Phase 1. Second, and more importantly, I worry that skeptical

reviewers will be concerned about Phase 1 (and the violation of the

preferences) somehow contaminating responses in the second phase or

making it less readily interpretable.

Don't feel pressured to combine them. Maybe you should just work on

the changes for the "pure", uncombined designs for now, and we can

then discuss them. Depending on the participant pool you use, it might

be possible to run each separately. In fact, if you get similar

responses, that's a nice robustness check, and if you get

systematically different responses, that might be theoretically

interesting. Either with Prolfic or through one of my contact

organizations, doing both could certainly be an option, especially as

these are relatively short tasks for the participants.

May 1, 2025

Hi Ken,

This was developed by tomorrow's seminar speaker:

[https://blogs.lse.ac.uk/impactofsocialsciences/2024/10/30/ai-can-carry-out-qualitative-research-at-unprecedented-scale/](https://blogs.lse.ac.uk/impactofsocialsciences/2024/10/30/ai-can-carry-out-qualitative-research-at-unprecedented-scale/)

It's a tool to make it possible to conduct thousands of interviews

within hours with the help of an AI chatbot (that is rated as well as

an expert interviewer). It's easy to integrate it with Prolific. And

respondents feel it captures their thoughts well, and they tend to

write significantly more words in their responses to the chatbot

compared to open text fields.

It could be an interesting (and easy?) possible complement to your

data collection through a cool new approach. You could try to use it

to get people's thoughts/evaluations on AI vs. human-produced outputs.

András

April 29, 2025

BACKGROUND: [Andras notes on comps April 25, 2025](https://www.notion.so/Andras-notes-on-comps-1e0719e9ff3880a7a2ccf4fb341c4dd3?pvs=21)

Hi Ken,

It was great to chat with you this afternoon.

As I mentioned, it's a very interesting idea to explore the possibility of using the current outcome #3 as an incentive-compatible survey item and perhaps using it as the main outcome.

On a more abstract level, the difference in design relates to what you would study conceptually: an evaluation (with a pure label effect) of a given output versus a revealed preference for the author/source of an (anticipated but not yet seen, but still consequential) output. So the original approach is more about an evaluation; the potential new approach is more about expected quality. This may have some implications of how you write this up.

In some ways, this new approach would simplify the design (as we discussed), make it easier to scale up quickly across various contexts (e.g., HR decisions, strategic choices, technology procurement), and make it possible to include AI-human collaboration as one of the options _within_ each report section.

One potential issue is that this approach doesn't actually involve randomization, so some people might consider this an incentive-compatible survey rather than an experiment. That's not necessarily a fatal problem, but something to be aware of. Sometimes people expect different things from surveys (e.g., be "representative" of some population of interest, though not necessarily) vs. experiments (e.g., causal identification within a sample).  Another issue (that we haven't discussed as much in the context of the new approach) is that this approach might make it quite obvious what you want to study.

To address the issue that this makes it too obvious what you're studying, you could add several plausible but low-impact "filler" choice. (e.g., [here](https://doi.org/10.1177/0003122420969399), I used some filler materials and filler questions to mask the true purpose of the study). That's often a good way to mask your key manipulations and avoid participants guessing your hypothesis (which could otherwise lead to demand effects). You could tell participants something like this: "Equity research reports vary along many dimensions. Please indicate your preferences for the following features of the report you would like to review before you make your investment decision." Then, in addition to (i.e., both before and after) the section author preferences (AI, human, AI-human), you could elicit their preferences for a few other things, e.g., the equity research firm whose analyst/AI is responsible for the study (you could show them names/logos, maybe even descriptions: "Delta Capital," "Summit Research," "Evergreen Partners") and some similar things. And shuffle the order of choosable attributes for different participants (so "Source/Author" of sections is not always #1) or make the filler attributes look equally emphasized to avoid making your real focus too salient. (e.g.,  Which font style do you find easier to read? Arial / Calibri / Times New Roman; Which layout do you prefer? Single-column format? Two-column format; Would you like the report to feature a small "Key Points" sidebar / summary box? Yes/ No / PDF report vs. web-based report, etc.). And then: Click Submit to submit your preferences. _After you submit, you will be presented with reports that match your selected preferences as closely as possible. You can then make your investment decision_. The name of the firm is a good filler because it's low-impact (if none of the firms seem better/more elite) and also because it doesn't mean you need to change anything for the participants post-choice (i.e., you could give them the same report without saying what firm it came from).

If we're worried about this not being a randomized experiment, you could also consider a mix of the two approaches. People are told they have to make an incentivized decision. They are then told that they can choose the report can use in making their decision.  So far, it's the same as the potential new approach. But then, instead of choosing from a list of features (e.g., author of information: AI/human/etc., name of equity research firm, etc.), you give them a choice of two reports. (Or, perhaps, two "test reports" and one filler report, as in [Benard and Castilla 2010](https://journals.sagepub.com/doi/abs/10.2189/asqu.2010.55.4.543?casa_token=7EwJP-GI1zsAAAAA:Z5y-8oNusFjH39OPMOBEH4dESqDF2Fk-aGtSUjYTSh4GcMHW6dp5TG2DiQfeeF1tnx2nj2vHPBI-), where they use two test candidates and one filler candidate, who is much weaker than the other two, so is never really chosen, but helps make it seem less obvious what the study is about). And then you give people two reports like:

Research Firm: InvestQ Research (randomized names)

Information by: Michael Smith, Analyst

Analysis by: InvestQ's AI Assistant

Recommendation by: Michael Smith, Analyst

[Some other fillers perhaps, which you randomize]

Research Firm: BlueSky Analysis (randomized names)

Information by: James Davis, Analyst

Analysis by: James Davis, Analyst

Recommendation by:  James Davis, Analyst

[Some other fillers perhaps, which you randomize]

{Plus a filler that people wouldn't chooose (e.g., because it's a very outdated report relative to the others), maybe, though it's not entirely necessary, and maybe not that easy in this context.)

And then people choose one of the reports. So, this **combines** **the factorial experiment aspects of the original design** (including randomization) **with a nice feature of the potential new design** (i.e., it makes the outcome very clearly tied to what you want to measure: a revealed preference for, or revealed trust in, an AI or human author). And because the key decision is made before people see the report, you don't need to worry about things like "Will people believe this is AI/human generated? Will people notice my manipulation embedded in a report? etc." You don't have to worry about those because the experimental choice is made before the investment decision. And it's directly made for a report with particular authors/features, which makes it easily comparable across contexts (e.g., HR, finance, technology choice). And this is still a randomized experiment, a familiar format for most people.

So maybe that's the way to do this? The only thing this is less good at than the incentive-compatible surve is that, because it's a factorial survey, including 3 conditions (AI, human, AI&human) would make for 3 x 3 x 3 = 27 cells. So maybe 2 x 2 x 2 is more feasible with this approach.

András

One more thing: Overall,  I think you have a few options here: (a) the original factorial experiment (with some tweaks per my written comments on your document); (b) the incentive-compatible, revealed-preferences survey (with some “filler” features to choose from) as we discussed on Zoom, which has a lot of nice aspects, too; or (c) some mix of the two (e.g., as I mentioned in my email) if you can figure out how exactly to have a design that includes some randomization but also measures (and makes it possible to easily analyze) a preference for source/author directly. For example, with (c), one option may be to let people choose from two reports (including some randomly varied incidental/inconsequential/filler features) but set things up so that one report is always all human-authored (i.e., basically, what we'd think of as a traditional report), on all three sections, while the other has some AI use but varies in which section(s) AI was used). That way, you could analyze which type/level of AI usage (of the seven possible combinations that use at least some AI) is more likely to beat the traditional report, right?

Anyway, you have options and an interesting project. So sleep on it, think about it, perhaps get some advice from Tanjim, too, and see you can find some helpful precedent/template, too. And then let's discuss. :)

April 25, 2025

Dear Ken,

Thank you for sharing your paper. This is a timely and exciting project with real potential for contributions. Below are some thoughts that I hope will be useful as you continue developing the work:

1. Framing and Motivation: While structured reports are a fine empirical focus, I would encourage you to frame the paper more broadly from the outset. Organizations produce a wide range of consequential written materials, and how audiences evaluate these materials is a critical issue—one that management and organizational scholars have long studied in the pre-AI world. What is new, and still underexplored, is how AI is changing those dynamics. While there is growing research on whether AI can match or exceed human-generated output, the question of _how audiences evaluate_ AI-produced materials, especially when authorship is disclosed, remains unresolved. As you note, there are conflicting perspectives and results regarding this.
2. Three-Part Structure as Part of Your Story: One of your key conceptual contributions is the observation that many important written outputs produced in and by organizations contain _three distinct elements_. That insight is conceptually useful and not something most prior studies on evaluations of AI/human outputs have explicitly acknowledged. So, I would not give it away too early in the paper. Instead, consider building up to it and presenting it as part of how your paper helps explain the mixed findings in prior work. It's part of your answer, rather than a part of your question.
3. Expert vs. Layperson Evaluation: You make an interesting point about whether the audience consists of experts or laypeople. I had not thought about this as a potential moderator, but it could be important—especially since different types of structured reports may be intended primarily for experts (e.g., investor analysts) or for laypeople, or for both. This distinction may have implications for both your theoretical framing and your empirical design. You could consider tailoring recruitment based on the audience you want to study.
4. Other Sources of Heterogeneity: Your observation above also made me wonder whether other forms of heterogeneity might be worth exploring or at least measuring. For example, there is a literature on how gender affects openness to new technologies. You may not want to center that in your argument, but it could be useful to collect such data for exploratory purposes. Having data on demographic and other individual characteristics and exploring (and, at least, ruling out) their potential moderating effect could also help you respond to reviewer questions along the lines of, "But does this apply to everyone equally? How about women? People who use AI themselves regularly? Domain experts? More versus less educated folks? Etc."
5. Building Up the Null and Counterhypotheses: As you develop your hypotheses, I would encourage you to think carefully about both the null (why the source/author might _not_ matter) and plausible counterhypotheses. For instance, some participants may prefer AI authorship in recommendations, believing it to be more objective or less fallible, conditional on a given set of data. Or people may, in fact, discount AI-authored data/analysis due to concerns about hallucinations or lack of human contextual/analytic judgment. Ezra Zuckerman’s point here is useful: if you cannot articulate a somewhat reasonable alternative or null, the hypothesis may not be very interesting. Including (and addressing) these perspectives will also help make your theoretical section richer. It will also help you interpret results in case they go against your expectations. And, importantly, thinking through this will help you come up with a list of mechanisms that you could try to measure.
6. Mechanisms: As you know, one of the most common reviewer questions we get, in almost every type of empirical research, is: "Okay, interesting effect, but what is the _mechanism_?" When you work on designing the experiment, think about how you will measure potential mechanisms — and what they might be (e.g., everything from concerns about contextual judgment to concerns about hallucinations, etc.).
7. Investment Task Design: If you are concerned that participants' prior familiarity with certain companies might overwhelm the author-label effect, one option is to use reports about foreign firms traded on non-U.S. exchanges. That could reduce baseline familiarity, though it also raises the question of whether participants will interpret AI use differently in a foreign context. It's a trade-off worth thinking through.
8. Hiring Task Design: For the hiring recommendation task, one way to increase realism and create an incentive-compatible setup is to use actual performance trajectory data from a partner organization or from existing datasets. For example, I still have data from my 2014 _ASQ_ paper, which includes multi-year performance ratings for a cohort of new hires (some who ended up doing well and others who ended up doing poorly over the following 5-10 years, but all started at the same time). We can discuss whether something like that might work for your purposes.
9. Participant Recruitment: A key practical issue will be sourcing the right participants. For example, responses from people who have never seen an analyst report or never made investment decisions may not be very informative. On platforms like Prolific, you can use pre-screening surveys to identify individuals with relevant experience (e.g., who have read analyst reports or invested in individual stocks with some regularity). Similarly, for the hiring scenario, it may be possible to recruit HR professionals or experienced managers. I also have contacts at a couple of large HR and recruiter professional associations in the U.S. and might be able to help through connections there. Some reviewers appreciate such participants much more than Prolific participants.

Overall, this is shaping up to be a thoughtful and well-positioned project. You are asking important questions, and you are approaching them in a way that can generate theoretical insights. Looking forward to the next steps!

András

April 14, 2025

Hi Ken,

A few quick comments:

1. I like the idea of thinking about how to make the disclosure of AI

use seem realistic, and this seems to be one plausible way of doing

that.

One concern about using a student/educational context might be that

(a) the context might not feel as organizational/managerial as it

could (as it feels more educational) and, (b) it might be that

participants will wonder about how honest students are, which could

muddy the waters (e.g., make the manipulations less clean if people

don't necessarily believe that students didn't use AI). There is a lot

of discourse about cheating with AI (and hiding the use of AI) in

higher education.

I guess another approach would be to do something similar to what you

suggest but make the reports seem professional (i.e., from an analyst

at a firm) but mention upfront that one of the transparency policies

of the firm is that analysts must disclose AI use in their reports and

how exactly it was used.

On those policies, see:

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.thomsonreuters.com%2Fen-us%2Fposts%2Ftechnology%2Fchatgpt-wall-street-research%2F&data=05|02|kenxj.zhang%40rotman.utoronto.ca|5ac7aa12da7b4ea5b34d08dd7b0d4f84|78aac2262f034b4d9037b46d56c55210|0|0|638802019299693689|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|0|||&sdata=A4bWgvNMoN36G3CB4KnHJVo8gBEkcKDxnF8DQraBHE0%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.thomsonreuters.com%2Fen-us%2Fposts%2Ftechnology%2Fchatgpt-wall-street-research%2F&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5ac7aa12da7b4ea5b34d08dd7b0d4f84%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638802019299693689%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=A4bWgvNMoN36G3CB4KnHJVo8gBEkcKDxnF8DQraBHE0%3D&reserved=0)

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Feconomictimes.indiatimes.com%2Fmarkets%2Fstocks%2Fnews%2Finvestment-advisers-research-analysts-should-disclose-ai-tool-usage-to-clients-sebi%2Farticleshow%2F112544450.cms%3Ffrom%3Dmdr&data=05|02|kenxj.zhang%40rotman.utoronto.ca|5ac7aa12da7b4ea5b34d08dd7b0d4f84|78aac2262f034b4d9037b46d56c55210|0|0|638802019299710881|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|0|||&sdata=6%2BlDUO1%2F8Ry2t4%2F3klBe0fyknIpV3GsCb%2BLwL1Xccuo%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Feconomictimes.indiatimes.com%2Fmarkets%2Fstocks%2Fnews%2Finvestment-advisers-research-analysts-should-disclose-ai-tool-usage-to-clients-sebi%2Farticleshow%2F112544450.cms%3Ffrom%3Dmdr&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5ac7aa12da7b4ea5b34d08dd7b0d4f84%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638802019299710881%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=6%2BlDUO1%2F8Ry2t4%2F3klBe0fyknIpV3GsCb%2BLwL1Xccuo%3D&reserved=0)

And another advantage of this is that it's perhaps more portable to

other contexts (e.g., a hiring/work assignment recommendation from an

HR analyst, etc.)

2. I like the idea of a single authorship condition per participant.

It's also nice to have them rate multiple things to increase power but

you also need to balance that against the concern that the task

becomes too long (and thus people don't do it or only do it very

expensively). So showing them one manipulated report could also be an

option. Depending on the setting, it's often much easier to have 400

people take a 5-minute study than to get 200 people to take a

10-minute study.

What you write up for the classes doesn't need to be the final

version, of course, so go with whatever seems to make sense the most

at this point, and we can continue to iterate later, too. And, of

course, ask Tanjim about the design.

Best,

András

P.S. This might be interesting to look at:

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.osc.ca%2Fen%2Fnews-events%2Fnews%2Fosc-study-examines-role-artificial-intelligence-retail-investing&data=05|02|kenxj.zhang%40rotman.utoronto.ca|5ac7aa12da7b4ea5b34d08dd7b0d4f84|78aac2262f034b4d9037b46d56c55210|0|0|638802019299722663|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|0|||&sdata=%2FMWf5cTvCk8arsbM2KJWGB8VtTn5rlfgOjJmNHP286w%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.osc.ca%2Fen%2Fnews-events%2Fnews%2Fosc-study-examines-role-artificial-intelligence-retail-investing&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C5ac7aa12da7b4ea5b34d08dd7b0d4f84%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638802019299722663%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=%2FMWf5cTvCk8arsbM2KJWGB8VtTn5rlfgOjJmNHP286w%3D&reserved=0)

April 9, 2025

Hi Ken,

Good to see you today!

I was just wondering if you had considered a kind of conjoint

experiment as one possibility for the AI study -- or perhaps an

extended iteration of it down the road. Similar to Manuela's

harassment paper (not sure if you know it but you can ask her for it;

she can probably recommend other good ones, too) or the attached

paper. But, of course, you'd want to make the outcome

incentivized/behavioral rather than just a stated preference.

You could have an "attribute" called "authorship of factual

description," and it could have three values (human, AI, or human-AI

collaboration). Then, a second attribute would be "authorship of

analysis," with again three values. And the same for the third

attribute, i.e., "authorship of recommendation." And you would include

several more attributes mostly/partly as fillers to make sure it's not

obvious what you're studying. And then have people choose between

which full report they want to see (in a way that the choice matters,

e.g., they can use it in an investment or hiring simulation where real

money is at stake).

The attached paper goes through a bunch of possible advantages of this

approach, though their DV isn't incentivized.

Anyway, just something to consider. The design you mentioned today is

also promising, so you have options. :)

András

April 2, 2025

Hi Ken,

I came across these two very relevant papers for us. Take a look at my quick summaries below. I am also attaching the papers. :)

**The Effort Heuristic:**

[](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdoi.org%2F10.1016%2FS0022-1031\(03\)00065-9&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C6b154c7f95c643d976f408dd71ff8156%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638792064728151986%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=%2B%2F9x%2FyBqswVzDOiw1NPGgb8pYWlVev3orH8kzu8LQhA%3D&reserved=0)[https://doi.org/10.1016/S0022-1031(03)00065-9](https://doi.org/10.1016/S0022-1031\(03\)00065-9)

- People use effort as a heuristic for quality. "Participants rating a poem (Experiment 1), a painting (Experiment 2), or a suit of armor (Experiment 3) provided higher ratings of quality, value, and liking for the work the more time and effort they thought it took to produce."  --> This seems very relevant for us, as AI doesn't really make an "effort" in the human sense. And also, human-AI collaboration might be seen as requiring less effort than a fully human-made production.

**Art and Authenticity:** [](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fpsycnet.apa.org%2Fbuy%2F2011-25897-001&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C6b154c7f95c643d976f408dd71ff8156%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638792064728174172%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=Tlk775lx%2BqqNLOAffTmTEPkr299IeARc4T0OdEgAvkg%3D&reserved=0)[https://psycnet.apa.org/buy/2011-25897-001](https://psycnet.apa.org/buy/2011-25897-001). - People prefer original art over forgeries for two reasons. First, the original art is seen as the product of a unique creative performance (i.e., people care about the creative process whereby the art was created). Second, there is a possible contagion effect: people value the fact that the created object came into direct physical contact with the artist. --> This, too, seems relevant for us, for these two mechanisms don't really play in the case of fully AI-made content.

Best,

András

Hi Ken,

No need to look at this now. This is just for later.

I fed ChatGPT 4.5 a bunch of the ideas we discussed and the papers I recently found. It came up with the following after some back and forth. This is long so you can look at it later, but it's worth a look.

One new idea that came out of this is the role of _status_ (point #3 under Theoretical Foundations), which is nice as it could allow us to connect to the organizational/sociology literature and make it less micro/OB/psych.

Another idea that actually came from me, but I had ChatGPT work on developing, is the possibility of three (not just two) broad categories of content type: 1. artistic/aesthetic content (e.g., poetry), 2. technical/functional content (e.g., news summary), and 3. an intermediate category (e.g., creative nonfiction, or a piece of furniture that has both functional value and aesthetic value).

András

---

## Theoretical Foundations

Your research touches several interrelated theoretical ideas:

### 1. **Authenticity and Craft (Effort Heuristic)**

- **Theory:**
    
    Humans often attribute higher value to content perceived to result from significant human effort, hardship, or intentionality (the "effort heuristic"). Perceived craft—reflecting personal investment, struggle, and skill—often enhances perceptions of authenticity and intrinsic value (Kruger, Wirtz, Van Boven, & Altermatt, 2004; Newman & Bloom, 2012).
    
- **Why it Matters for Your Study:**
    
    When evaluating art, poetry, or personal narratives, audiences may highly value the perceived "human struggle" behind the content. They see authenticity in the metaphorical or literal blood, sweat, and tears exerted by a creator. By contrast, AI-generated content might be devalued precisely because audiences perceive it as effortless or devoid of human emotional commitment.
    

---

### 2. **Mind Perception and Theory of Mind**

- **Theory:**
    
    According to the "mind perception" theory (Gray, Gray, & Wegner, 2007), people ascribe intentionality, emotion, and experiential states to human creators, allowing them to empathize, relate emotionally, and appreciate content through the lens of human experience.
    
- **Why it Matters for Your Study:**
    
    Audiences might inherently value human-made content more for artistic or deeply personal genres (like poetry or art) precisely because they imagine the author’s emotional or existential journey. AI-generated content lacks such emotional "backstories," thereby diminishing affective appreciation, regardless of objective quality.
    

---

### 3. **Signaling Theory & Status Consumption**

- **Theory:**
    
    Content evaluation can serve as social signaling. Appreciating and endorsing human craft signals discernment, taste, sophistication, or virtue (Berger & Ward, 2010). Endorsing human-produced content could thus represent a socially desirable trait—signaling moral standing, aesthetic sensibility, or solidarity with human creators.
    
- **Why it Matters for Your Study:**
    
    Participants might consciously or subconsciously align their preferences with content perceived as human-made (or at least collaborative) in genres that have strong artistic or moral dimensions, explicitly to signal desirable traits or identities.
    

---

### 4. **Essentialism and the Value of Human Creativity**

- **Theory:**
    
    Psychological essentialism (Gelman, 2003; Newman & Bloom, 2012) posits that humans intuitively value original objects or creations because they contain an intangible "essence"—a unique imprint or personal touch reflecting the human creator's uniqueness and irreproducibility.
    
- **Why it Matters for Your Study:**
    
    Audiences might devalue AI-generated content in genres traditionally associated with individual creativity or self-expression (e.g., poetry, art), as AI-produced items inherently lack this intangible essence. By contrast, human-AI collaborative content might partially preserve this essence, thus resulting in moderate evaluations.
    

---

### **Integrating these Ideas into Your Hypotheses**

You can theorize an interaction between content genre and authorship attribution grounded in these theoretical frameworks:

- **Poetry, Art, or Personal Narratives:**
    - Audiences value the perceived effort, authenticity, emotional depth, and craft of human authorship highly.
    - Hypothesis: **Fully human-made** content will receive significantly higher evaluations due to perceived authentic emotional labor ("blood, sweat, tears"), compared to fully AI-generated content.
    - Collaborative content might receive intermediate evaluations, seen as less authentic but intriguing due to the innovative combination of human insight and technological novelty.
- **News Summaries, Technical Writing, or Informational Content:**
    - Craft and emotional authenticity matter less; pragmatic values (accuracy, efficiency, reliability) predominate.
    - Hypothesis: Differences between human-made and AI-generated content evaluations diminish, as the primary evaluation criteria shift toward pragmatic qualities rather than emotional investment or craft.

---

### **Possible Mediating Mechanisms:**

You can explicitly test mechanisms including:

- **Perceived Craft & Effort:**
    
    "How much effort do you think went into creating this piece?"
    
    "How much emotional energy did the creator invest?"
    
- **Authenticity Perceptions:**
    
    "To what extent does this content feel authentic or genuine?"
    
- **Mind Perception:**
    
    "To what extent do you perceive the creator had genuine emotional experiences when creating this?"
    
- **Status Signaling (Social desirability):**
    
    "Would appreciating this content reflect positively on you in the eyes of others?"
    

Testing these mediators allows you to pinpoint the psychological processes driving evaluation differences across conditions.

---

### **Potential Outcomes and Contributions:**

Your study can uniquely contribute to understanding the evolving role of AI and human creators by clarifying:

- When and why human emotional investment, effort, and authenticity matter most (e.g., for poetry or art), and when they matter least.
- How collaboration with AI is perceived (possibly as combining the advantages of human emotional investment with AI efficiency and novelty).
- How your results inform practical implications for artists, content creators, publishers, and policymakers regarding transparency about AI-involvement in creative processes.

---

## **Content Type Categories**

## **1. Artistic/Aesthetic Category**

(High emotional authenticity, subjective meaning-making, strong perceived craft, creativity, and emotional investment)

- **Poetry**
    - Brief, emotionally expressive verse; highly dependent on perceived emotional authenticity.
- **Short Fiction**
    - Emotionally resonant narratives; audiences often value perceived human insight.
- **Digital Visual Art**
    - Artistic visuals or illustrations emphasizing aesthetics and creativity.
- **Song Lyrics**
    - Lyrics perceived as personal expressions with strong emotional narratives.
- **Personal Essay / Memoir Excerpts**
    - Highly subjective, reflective personal experiences emphasizing perceived human authenticity and emotional vulnerability.
- **Fine Art Photography**
    - Emotionally evocative photographic content perceived as reflecting artistic vision and human effort.

---

## **2. Intermediate (Hybrid) Category**

(Blending aesthetic/artistic and practical/utilitarian considerations, balancing creativity with clear functional value)

- **Creative Nonfiction / Narrative Journalism**
    - Nonfiction writing blending factual reporting with literary narrative style, emotional resonance, or vivid storytelling (e.g., New Yorker-style profiles, long-form journalism).
- **Interior Design / Furniture Descriptions**
    - Written descriptions emphasizing both aesthetics (craft, design creativity) and practical functionality (usability, ergonomics).
- **Architectural Plans or Descriptions**
    - Text-based descriptions highlighting creative vision and aesthetic appeal alongside practical, structural functionality and usability.
- **Food Writing / Recipes**
    - Culinary content blending aesthetic creativity (flavor descriptions, sensory imagery) with practical instructional value (precise cooking directions).
- **Travel Writing**
    - Narratives blending factual descriptions of places with aesthetic experiences and vivid, sensory storytelling.
- **Fashion Descriptions (high-end clothing)**
    - Descriptions of apparel blending practical details (comfort, usability) with fashion creativity (style, artistic flair).

---

## **3. Technical/Nonartistic Category**

(Primary emphasis on efficiency, clarity, accuracy, objective information; low perceived craft/emotional involvement)

- **News Summaries**
    - Brief, objective accounts of current events emphasizing accuracy, clarity, and reliability.
- **Technical Writing (Manuals/Instructions)**
    - Clear step-by-step procedures emphasizing functionality and clarity.
- **Product Descriptions (everyday products)**
    - Functional, clear explanations highlighting usability, specifications, and practical benefits.
- **Scientific Abstracts**
    - Brief scientific summaries emphasizing precision, accuracy, and clarity.
- **Legal Documents (Simplified)**
    - Simple, practical legal instructions or summaries clearly emphasizing clarity, accuracy, and objectivity.
- **Financial Reports**
    - Objective, data-driven reports summarizing financial data, with minimal emotional or subjective considerations.

---

## **Why These Three Clearly Differentiated Categories?**

- **Theoretical Richness:**
    
    You can explicitly test how perceived human effort and authenticity differ across clearly distinct and intermediate genres.
    
- **Experimental Power:**
    
    Having multiple genres per category allows you to generalize your findings, ensuring robustness.
    
- **Insights into Boundary Conditions:**
    
    Intermediate genres let you examine how people balance and trade-off aesthetic/emotional criteria against pragmatic/functional criteria—valuable insight for understanding when AI collaboration may be viewed positively.
    

---

## **Possible Hypotheses About the Intermediate Category:**

- Audiences might value **human-AI collaborative content** highly in intermediate genres because it explicitly blends the strengths of human emotional/aesthetic authenticity with AI’s practical efficiency or precision.
- The intermediate category may show the smallest differences between conditions, or an interesting curvilinear effect (fully human or fully AI less favored compared to collaborative).

---

## **Additional Considerations for Operationalizing the Intermediate Category:**

- **Creative nonfiction and narrative journalism:**
    
    Use short narrative excerpts clearly blending facts with artistic storytelling.
    
- **Furniture/Interior design descriptions:**
    
    Short textual descriptions explicitly mentioning both aesthetic style ("modern, sleek lines with natural warmth") and practical functionality ("ergonomic comfort, durable materials").
    
- **Architectural descriptions:**
    
    Brief, evocative written descriptions balancing visual aesthetic appeal ("minimalist elegance, harmonious design") and functional considerations ("efficient space use, energy-saving design").
    
- **Food writing:**
    
    Vivid descriptions blending sensory/emotional appeal ("aromatic spices, comforting warmth") with precise practical recipes or instructions.
    

---

## **Recommended Next Steps:**

- Choose your final selection of genres within each category.
- Ensure stimuli are comparable in quality across AI/human attribution conditions.
- Develop clear manipulations of authorship attributions.

---

### **Recommended Explicit Genres (Finalized Suggestion):**

|Artistic/Personal (Emotion/Aesthetic)|Intermediate (Hybrid)|Practical/Utilitarian|
|---|---|---|
|Poetry|Creative nonfiction|News summaries|
|Short fiction|Architectural descriptions|Technical instructions|
|Visual digital art|Interior/furniture design|Product descriptions|
|Song lyrics|Food writing/recipes|Scientific abstracts|
|Memoir / personal essay|Travel writing|Legal documents (simplified)|
|Fine art photography|Fashion descriptions|Financial reports|

This robust set of genres covers a meaningful range of human vs. AI valuation criteria, allowing clear tests of your theoretical propositions around craft, effort, authenticity, and utility.

Hi Ken,

Thanks for the interesting chat after class about this idea.

Why don't we set up a separate meeting to brainstorm and sketch out a possible design (once you had time to think about this)? When might be a good time for that?

Btw, I liked the potential connections you highlighted to organizational work (on nostalgia for "premodern" things, craft production, things like Aruna's work; I think there is also a connection to Ryan Raffaeli's [work](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.hbs.edu%2Fris%2FPublication%2520Files%2F2025-02-03%2520Raffaelli%2520and%2520Noe%2520Indie%2520Bookstores%2520Forthcoming%2520Admin%2520Sci%2520Quarterly_9dbecc71-e698-4407-948f-637893efc848.pdf&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C107f72dbdae74e147f8d08dd6afd416d%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638784358265034259%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=nR4Ue6UPWYStrf3ND7Qht1ZkuLQQEQKDf1hJlo3cWRs%3D&reserved=0), or even more broadly to Max Weber's idea of disenchantment, Marx's idea of alienation, but more from the demand side of evaluators rather than the producers).

I also like the idea of potentially broadening the scope from just written text (poetry, summary of a research paper, media digest, short story, non-fiction, business idea, an actual research paper, even a thread on social media, etc.) to additional things as well (e.g., digital art, architectural design, design of a sculpture, even a song, etc.). But, of course, for an experiment, we'd need a manageable number of categories that are theoretically interesting.

Here are some of the papers I mentioned:[](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.pnas.org%2Fdoi%2Fabs%2F10.1073%2Fpnas.2319112121&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C107f72dbdae74e147f8d08dd6afd416d%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638784358265077827%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=S88TK%2Bxr7ObYl4nVSPf0g%2Fy39zxvZsGWRKuVetxozLs%3D&reserved=0)[https://www.pnas.org/doi/abs/10.1073/pnas.2319112121](https://www.pnas.org/doi/abs/10.1073/pnas.2319112121)[](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS0747563223002261&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C107f72dbdae74e147f8d08dd6afd416d%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638784358265108301%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=4yWPOH36AodSw6lpK6ye6dy7p%2Bdj%2FYWHhbRcM4gvPUE%3D&reserved=0)[https://www.sciencedirect.com/science/article/pii/S0747563223002261](https://www.sciencedirect.com/science/article/pii/S0747563223002261)[](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Facademic.oup.com%2Fpnasnexus%2Farticle%2F3%2F10%2Fpgae403%2F7795946&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C107f72dbdae74e147f8d08dd6afd416d%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638784358265132204%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=ys6N2Fg3qbuQX6CY6tInic1d3g0Bskpwj3KQo6wmq9A%3D&reserved=0)[https://academic.oup.com/pnasnexus/article/3/10/pgae403/7795946](https://academic.oup.com/pnasnexus/article/3/10/pgae403/7795946)[](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs44271-024-00182-6&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C107f72dbdae74e147f8d08dd6afd416d%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638784358265153598%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=rli0NGVnZVMw7DrGPUXjiEkYcEquUuZv4aUu9wwmibo%3D&reserved=0)[https://www.nature.com/articles/s44271-024-00182-6](https://www.nature.com/articles/s44271-024-00182-6)[](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fjournals.sagepub.com%2Fdoi%2Ffull%2F10.1177%2F0003122414524575&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C107f72dbdae74e147f8d08dd6afd416d%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638784358265174774%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=Kp%2FsD%2BjTLd4xzYUUl9tL4HFjZepkgdz454ov9xBywcc%3D&reserved=0)[https://journals.sagepub.com/doi/full/10.1177/0003122414524575](https://journals.sagepub.com/doi/full/10.1177/0003122414524575)[](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fpsycnet.apa.org%2Frecord%2F2003-06417-001&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7C107f72dbdae74e147f8d08dd6afd416d%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638784358265196190%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=aP%2FTmlW09WYM%2Fc6y7SkUwYdROiGkIC%2FtGvXvkG%2BbDVw%3D&reserved=0)[https://psycnet.apa.org/record/2003-06417-001](https://psycnet.apa.org/record/2003-06417-001)

Best,

András

Hi Ken,

I hope you're doing well!

I am sorry for the delayed response; I've been a bit overwhelmed with

end-of-semester deadlines, but things should get better soon. I look

forward to figuring out what we could do with this idea.

How is 10:15 - 11:30, or 2:15 - 3:30 on Thursday at Rotman?

Alternatively, Wednesday at my house also works, at any time (I'll be

working from home that day, so if you want to drop by for a morning

snack or afternoon tea and talk through ideas in a more relaxed

setting, that works, too.)

Best,

András

P.S. Two more interesting snippets with some relevance that I recently

came across:

1. Short video: Hayao Miyazaki's reaction to seeing an AI-generated

animation in 2016: “I am utterly disgusted […] I strongly feel that

this is an insult to life itself." -

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fx.com%2FToonHive%2Fstatus%2F1905277956064149723&data=05|02|kenxj.zhang%40rotman.utoronto.ca|fbe0a354988f4a568cec08dd6ee063ce|78aac2262f034b4d9037b46d56c55210|0|0|638788632221523411|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|0|||&sdata=gCurVP3S0wLB22XOP5kygBC5NcCOhRD3rGm1vp7JGkc%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fx.com%2FToonHive%2Fstatus%2F1905277956064149723&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7Cfbe0a354988f4a568cec08dd6ee063ce%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638788632221523411%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=gCurVP3S0wLB22XOP5kygBC5NcCOhRD3rGm1vp7JGkc%3D&reserved=0)

2. Long-form article: "You Are Not a Parrot And a chatbot is not a

human. And a linguist named Emily M. Bender is very worried what will

happen when we forget this." --

[https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fnymag.com%2Fintelligencer%2Farticle%2Fai-artificial-intelligence-chatbots-emily-m-bender.html&data=05|02|kenxj.zhang%40rotman.utoronto.ca|fbe0a354988f4a568cec08dd6ee063ce|78aac2262f034b4d9037b46d56c55210|0|0|638788632221539507|Unknown|TWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D|0|||&sdata=f9ABfOINpg0j6d0jWs9FbozzQZA8fyMjfSTLq%2FRVV%2BQ%3D&reserved=0](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fnymag.com%2Fintelligencer%2Farticle%2Fai-artificial-intelligence-chatbots-emily-m-bender.html&data=05%7C02%7Ckenxj.zhang%40rotman.utoronto.ca%7Cfbe0a354988f4a568cec08dd6ee063ce%7C78aac2262f034b4d9037b46d56c55210%7C0%7C0%7C638788632221539507%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=f9ABfOINpg0j6d0jWs9FbozzQZA8fyMjfSTLq%2FRVV%2BQ%3D&reserved=0)